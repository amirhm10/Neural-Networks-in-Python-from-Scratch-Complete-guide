{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network for scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "inputs = iris.data\n",
    "outputs = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape,outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test, y_train, y_test = train_test_split(inputs, outputs,\n",
    "                                                 test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 4), (120,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MLPClassifier(max_iter = 2000,\n",
    "                        verbose=True,\n",
    "                        tol=.000001,\n",
    "                        activation= \"logistic\",\n",
    "                        solver=\"adam\",\n",
    "                        learning_rate=\"constant\",\n",
    "                        learning_rate_init=.001,\n",
    "                        batch_size=10,\n",
    "                        hidden_layer_sizes = (4,5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.13977691\n",
      "Iteration 2, loss = 1.13290008\n",
      "Iteration 3, loss = 1.12606386\n",
      "Iteration 4, loss = 1.12127826\n",
      "Iteration 5, loss = 1.11780310\n",
      "Iteration 6, loss = 1.11289932\n",
      "Iteration 7, loss = 1.10913646\n",
      "Iteration 8, loss = 1.10689314\n",
      "Iteration 9, loss = 1.10473754\n",
      "Iteration 10, loss = 1.10232148\n",
      "Iteration 11, loss = 1.10029297\n",
      "Iteration 12, loss = 1.09907325\n",
      "Iteration 13, loss = 1.09752912\n",
      "Iteration 14, loss = 1.09639295\n",
      "Iteration 15, loss = 1.09453387\n",
      "Iteration 16, loss = 1.09371713\n",
      "Iteration 17, loss = 1.09238826\n",
      "Iteration 18, loss = 1.09103243\n",
      "Iteration 19, loss = 1.09018625\n",
      "Iteration 20, loss = 1.08886113\n",
      "Iteration 21, loss = 1.08813535\n",
      "Iteration 22, loss = 1.08612051\n",
      "Iteration 23, loss = 1.08484124\n",
      "Iteration 24, loss = 1.08367539\n",
      "Iteration 25, loss = 1.08183589\n",
      "Iteration 26, loss = 1.08078570\n",
      "Iteration 27, loss = 1.07848539\n",
      "Iteration 28, loss = 1.07656029\n",
      "Iteration 29, loss = 1.07450723\n",
      "Iteration 30, loss = 1.07274915\n",
      "Iteration 31, loss = 1.07021794\n",
      "Iteration 32, loss = 1.06766138\n",
      "Iteration 33, loss = 1.06502368\n",
      "Iteration 34, loss = 1.06246100\n",
      "Iteration 35, loss = 1.05927454\n",
      "Iteration 36, loss = 1.05635964\n",
      "Iteration 37, loss = 1.05281613\n",
      "Iteration 38, loss = 1.04911074\n",
      "Iteration 39, loss = 1.04562891\n",
      "Iteration 40, loss = 1.04207736\n",
      "Iteration 41, loss = 1.03795557\n",
      "Iteration 42, loss = 1.03302165\n",
      "Iteration 43, loss = 1.02852855\n",
      "Iteration 44, loss = 1.02403546\n",
      "Iteration 45, loss = 1.01919066\n",
      "Iteration 46, loss = 1.01337936\n",
      "Iteration 47, loss = 1.00807851\n",
      "Iteration 48, loss = 1.00240446\n",
      "Iteration 49, loss = 0.99625590\n",
      "Iteration 50, loss = 0.99050171\n",
      "Iteration 51, loss = 0.98387771\n",
      "Iteration 52, loss = 0.97678202\n",
      "Iteration 53, loss = 0.96954457\n",
      "Iteration 54, loss = 0.96234009\n",
      "Iteration 55, loss = 0.95426503\n",
      "Iteration 56, loss = 0.94669629\n",
      "Iteration 57, loss = 0.93807115\n",
      "Iteration 58, loss = 0.92932356\n",
      "Iteration 59, loss = 0.92033064\n",
      "Iteration 60, loss = 0.91130678\n",
      "Iteration 61, loss = 0.90174977\n",
      "Iteration 62, loss = 0.89206165\n",
      "Iteration 63, loss = 0.88267380\n",
      "Iteration 64, loss = 0.87211783\n",
      "Iteration 65, loss = 0.86218325\n",
      "Iteration 66, loss = 0.85212206\n",
      "Iteration 67, loss = 0.84147620\n",
      "Iteration 68, loss = 0.83164878\n",
      "Iteration 69, loss = 0.82117092\n",
      "Iteration 70, loss = 0.81098825\n",
      "Iteration 71, loss = 0.80065454\n",
      "Iteration 72, loss = 0.79053530\n",
      "Iteration 73, loss = 0.78088428\n",
      "Iteration 74, loss = 0.77115222\n",
      "Iteration 75, loss = 0.76155531\n",
      "Iteration 76, loss = 0.75223245\n",
      "Iteration 77, loss = 0.74307639\n",
      "Iteration 78, loss = 0.73420872\n",
      "Iteration 79, loss = 0.72542006\n",
      "Iteration 80, loss = 0.71695816\n",
      "Iteration 81, loss = 0.70871067\n",
      "Iteration 82, loss = 0.70079995\n",
      "Iteration 83, loss = 0.69304094\n",
      "Iteration 84, loss = 0.68566375\n",
      "Iteration 85, loss = 0.67851117\n",
      "Iteration 86, loss = 0.67109481\n",
      "Iteration 87, loss = 0.66451234\n",
      "Iteration 88, loss = 0.65779740\n",
      "Iteration 89, loss = 0.65142528\n",
      "Iteration 90, loss = 0.64538050\n",
      "Iteration 91, loss = 0.63956115\n",
      "Iteration 92, loss = 0.63396640\n",
      "Iteration 93, loss = 0.62836992\n",
      "Iteration 94, loss = 0.62292563\n",
      "Iteration 95, loss = 0.61779122\n",
      "Iteration 96, loss = 0.61344659\n",
      "Iteration 97, loss = 0.60809850\n",
      "Iteration 98, loss = 0.60357775\n",
      "Iteration 99, loss = 0.59935369\n",
      "Iteration 100, loss = 0.59499614\n",
      "Iteration 101, loss = 0.59078864\n",
      "Iteration 102, loss = 0.58686938\n",
      "Iteration 103, loss = 0.58305484\n",
      "Iteration 104, loss = 0.57942339\n",
      "Iteration 105, loss = 0.57593166\n",
      "Iteration 106, loss = 0.57227811\n",
      "Iteration 107, loss = 0.56895184\n",
      "Iteration 108, loss = 0.56586314\n",
      "Iteration 109, loss = 0.56264785\n",
      "Iteration 110, loss = 0.55976834\n",
      "Iteration 111, loss = 0.55680808\n",
      "Iteration 112, loss = 0.55396439\n",
      "Iteration 113, loss = 0.55118224\n",
      "Iteration 114, loss = 0.54855946\n",
      "Iteration 115, loss = 0.54597829\n",
      "Iteration 116, loss = 0.54352796\n",
      "Iteration 117, loss = 0.54098656\n",
      "Iteration 118, loss = 0.53862557\n",
      "Iteration 119, loss = 0.53631245\n",
      "Iteration 120, loss = 0.53414281\n",
      "Iteration 121, loss = 0.53218102\n",
      "Iteration 122, loss = 0.52978370\n",
      "Iteration 123, loss = 0.52773906\n",
      "Iteration 124, loss = 0.52588238\n",
      "Iteration 125, loss = 0.52381524\n",
      "Iteration 126, loss = 0.52175371\n",
      "Iteration 127, loss = 0.51992883\n",
      "Iteration 128, loss = 0.51817154\n",
      "Iteration 129, loss = 0.51616986\n",
      "Iteration 130, loss = 0.51435092\n",
      "Iteration 131, loss = 0.51286809\n",
      "Iteration 132, loss = 0.51106168\n",
      "Iteration 133, loss = 0.50922150\n",
      "Iteration 134, loss = 0.50740276\n",
      "Iteration 135, loss = 0.50579956\n",
      "Iteration 136, loss = 0.50411307\n",
      "Iteration 137, loss = 0.50240954\n",
      "Iteration 138, loss = 0.50070444\n",
      "Iteration 139, loss = 0.49911728\n",
      "Iteration 140, loss = 0.49788529\n",
      "Iteration 141, loss = 0.49607910\n",
      "Iteration 142, loss = 0.49419803\n",
      "Iteration 143, loss = 0.49262275\n",
      "Iteration 144, loss = 0.49107039\n",
      "Iteration 145, loss = 0.48941205\n",
      "Iteration 146, loss = 0.48772894\n",
      "Iteration 147, loss = 0.48601197\n",
      "Iteration 148, loss = 0.48442869\n",
      "Iteration 149, loss = 0.48268271\n",
      "Iteration 150, loss = 0.48094726\n",
      "Iteration 151, loss = 0.47923023\n",
      "Iteration 152, loss = 0.47768282\n",
      "Iteration 153, loss = 0.47581512\n",
      "Iteration 154, loss = 0.47405081\n",
      "Iteration 155, loss = 0.47215347\n",
      "Iteration 156, loss = 0.47035404\n",
      "Iteration 157, loss = 0.46853185\n",
      "Iteration 158, loss = 0.46687001\n",
      "Iteration 159, loss = 0.46477393\n",
      "Iteration 160, loss = 0.46261717\n",
      "Iteration 161, loss = 0.46070488\n",
      "Iteration 162, loss = 0.45876412\n",
      "Iteration 163, loss = 0.45666026\n",
      "Iteration 164, loss = 0.45465675\n",
      "Iteration 165, loss = 0.45256989\n",
      "Iteration 166, loss = 0.45040634\n",
      "Iteration 167, loss = 0.44846875\n",
      "Iteration 168, loss = 0.44589015\n",
      "Iteration 169, loss = 0.44390303\n",
      "Iteration 170, loss = 0.44195393\n",
      "Iteration 171, loss = 0.43915208\n",
      "Iteration 172, loss = 0.43748349\n",
      "Iteration 173, loss = 0.43462063\n",
      "Iteration 174, loss = 0.43231845\n",
      "Iteration 175, loss = 0.43014214\n",
      "Iteration 176, loss = 0.42886217\n",
      "Iteration 177, loss = 0.42587385\n",
      "Iteration 178, loss = 0.42278309\n",
      "Iteration 179, loss = 0.42030691\n",
      "Iteration 180, loss = 0.41774926\n",
      "Iteration 181, loss = 0.41535224\n",
      "Iteration 182, loss = 0.41304001\n",
      "Iteration 183, loss = 0.41024084\n",
      "Iteration 184, loss = 0.40823503\n",
      "Iteration 185, loss = 0.40537567\n",
      "Iteration 186, loss = 0.40278270\n",
      "Iteration 187, loss = 0.40020076\n",
      "Iteration 188, loss = 0.39775411\n",
      "Iteration 189, loss = 0.39514213\n",
      "Iteration 190, loss = 0.39246825\n",
      "Iteration 191, loss = 0.39053212\n",
      "Iteration 192, loss = 0.38712748\n",
      "Iteration 193, loss = 0.38490464\n",
      "Iteration 194, loss = 0.38225168\n",
      "Iteration 195, loss = 0.37946420\n",
      "Iteration 196, loss = 0.37809262\n",
      "Iteration 197, loss = 0.37410177\n",
      "Iteration 198, loss = 0.37233935\n",
      "Iteration 199, loss = 0.36898643\n",
      "Iteration 200, loss = 0.36701461\n",
      "Iteration 201, loss = 0.36442512\n",
      "Iteration 202, loss = 0.36125586\n",
      "Iteration 203, loss = 0.35876917\n",
      "Iteration 204, loss = 0.35608700\n",
      "Iteration 205, loss = 0.35420919\n",
      "Iteration 206, loss = 0.35088140\n",
      "Iteration 207, loss = 0.34868515\n",
      "Iteration 208, loss = 0.34593314\n",
      "Iteration 209, loss = 0.34313357\n",
      "Iteration 210, loss = 0.34118320\n",
      "Iteration 211, loss = 0.33929508\n",
      "Iteration 212, loss = 0.33562866\n",
      "Iteration 213, loss = 0.33312715\n",
      "Iteration 214, loss = 0.33061754\n",
      "Iteration 215, loss = 0.32819636\n",
      "Iteration 216, loss = 0.32585312\n",
      "Iteration 217, loss = 0.32413244\n",
      "Iteration 218, loss = 0.32078214\n",
      "Iteration 219, loss = 0.31902993\n",
      "Iteration 220, loss = 0.31617569\n",
      "Iteration 221, loss = 0.31369414\n",
      "Iteration 222, loss = 0.31152283\n",
      "Iteration 223, loss = 0.30879085\n",
      "Iteration 224, loss = 0.30643107\n",
      "Iteration 225, loss = 0.30430583\n",
      "Iteration 226, loss = 0.30166870\n",
      "Iteration 227, loss = 0.29939145\n",
      "Iteration 228, loss = 0.29732271\n",
      "Iteration 229, loss = 0.29495202\n",
      "Iteration 230, loss = 0.29281525\n",
      "Iteration 231, loss = 0.29036469\n",
      "Iteration 232, loss = 0.28789079\n",
      "Iteration 233, loss = 0.28551122\n",
      "Iteration 234, loss = 0.28356653\n",
      "Iteration 235, loss = 0.28124643\n",
      "Iteration 236, loss = 0.27913052\n",
      "Iteration 237, loss = 0.27692337\n",
      "Iteration 238, loss = 0.27616803\n",
      "Iteration 239, loss = 0.27250104\n",
      "Iteration 240, loss = 0.27193487\n",
      "Iteration 241, loss = 0.26834780\n",
      "Iteration 242, loss = 0.26621073\n",
      "Iteration 243, loss = 0.26426869\n",
      "Iteration 244, loss = 0.26205008\n",
      "Iteration 245, loss = 0.26049063\n",
      "Iteration 246, loss = 0.25786377\n",
      "Iteration 247, loss = 0.25596634\n",
      "Iteration 248, loss = 0.25521989\n",
      "Iteration 249, loss = 0.25261755\n",
      "Iteration 250, loss = 0.25032065\n",
      "Iteration 251, loss = 0.24811870\n",
      "Iteration 252, loss = 0.24634365\n",
      "Iteration 253, loss = 0.24460651\n",
      "Iteration 254, loss = 0.24250178\n",
      "Iteration 255, loss = 0.24079807\n",
      "Iteration 256, loss = 0.23896253\n",
      "Iteration 257, loss = 0.23690889\n",
      "Iteration 258, loss = 0.23560342\n",
      "Iteration 259, loss = 0.23323567\n",
      "Iteration 260, loss = 0.23151923\n",
      "Iteration 261, loss = 0.22972970\n",
      "Iteration 262, loss = 0.22832009\n",
      "Iteration 263, loss = 0.22693688\n",
      "Iteration 264, loss = 0.22614918\n",
      "Iteration 265, loss = 0.22319892\n",
      "Iteration 266, loss = 0.22139156\n",
      "Iteration 267, loss = 0.21977796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 268, loss = 0.21790308\n",
      "Iteration 269, loss = 0.21669757\n",
      "Iteration 270, loss = 0.21512492\n",
      "Iteration 271, loss = 0.21308435\n",
      "Iteration 272, loss = 0.21168982\n",
      "Iteration 273, loss = 0.21001012\n",
      "Iteration 274, loss = 0.20843610\n",
      "Iteration 275, loss = 0.20709002\n",
      "Iteration 276, loss = 0.20548107\n",
      "Iteration 277, loss = 0.20420760\n",
      "Iteration 278, loss = 0.20394951\n",
      "Iteration 279, loss = 0.20194343\n",
      "Iteration 280, loss = 0.19966939\n",
      "Iteration 281, loss = 0.19829243\n",
      "Iteration 282, loss = 0.19745340\n",
      "Iteration 283, loss = 0.19572629\n",
      "Iteration 284, loss = 0.19404674\n",
      "Iteration 285, loss = 0.19378304\n",
      "Iteration 286, loss = 0.19180777\n",
      "Iteration 287, loss = 0.19018647\n",
      "Iteration 288, loss = 0.18882151\n",
      "Iteration 289, loss = 0.18897738\n",
      "Iteration 290, loss = 0.18630962\n",
      "Iteration 291, loss = 0.18570775\n",
      "Iteration 292, loss = 0.18372607\n",
      "Iteration 293, loss = 0.18241431\n",
      "Iteration 294, loss = 0.18112141\n",
      "Iteration 295, loss = 0.18000360\n",
      "Iteration 296, loss = 0.17878479\n",
      "Iteration 297, loss = 0.17794332\n",
      "Iteration 298, loss = 0.17673595\n",
      "Iteration 299, loss = 0.17514169\n",
      "Iteration 300, loss = 0.17404834\n",
      "Iteration 301, loss = 0.17282695\n",
      "Iteration 302, loss = 0.17178011\n",
      "Iteration 303, loss = 0.17092907\n",
      "Iteration 304, loss = 0.17046085\n",
      "Iteration 305, loss = 0.16854360\n",
      "Iteration 306, loss = 0.16771402\n",
      "Iteration 307, loss = 0.16636802\n",
      "Iteration 308, loss = 0.16665556\n",
      "Iteration 309, loss = 0.16551461\n",
      "Iteration 310, loss = 0.16340724\n",
      "Iteration 311, loss = 0.16365092\n",
      "Iteration 312, loss = 0.16227960\n",
      "Iteration 313, loss = 0.16073776\n",
      "Iteration 314, loss = 0.15924261\n",
      "Iteration 315, loss = 0.15873582\n",
      "Iteration 316, loss = 0.15732754\n",
      "Iteration 317, loss = 0.15720925\n",
      "Iteration 318, loss = 0.15617946\n",
      "Iteration 319, loss = 0.15458231\n",
      "Iteration 320, loss = 0.15353943\n",
      "Iteration 321, loss = 0.15282729\n",
      "Iteration 322, loss = 0.15181696\n",
      "Iteration 323, loss = 0.15109830\n",
      "Iteration 324, loss = 0.15067779\n",
      "Iteration 325, loss = 0.14945284\n",
      "Iteration 326, loss = 0.14838938\n",
      "Iteration 327, loss = 0.14799311\n",
      "Iteration 328, loss = 0.14740484\n",
      "Iteration 329, loss = 0.14600970\n",
      "Iteration 330, loss = 0.14524699\n",
      "Iteration 331, loss = 0.14532445\n",
      "Iteration 332, loss = 0.14340643\n",
      "Iteration 333, loss = 0.14338189\n",
      "Iteration 334, loss = 0.14265252\n",
      "Iteration 335, loss = 0.14116517\n",
      "Iteration 336, loss = 0.14113257\n",
      "Iteration 337, loss = 0.14052891\n",
      "Iteration 338, loss = 0.13936271\n",
      "Iteration 339, loss = 0.13908055\n",
      "Iteration 340, loss = 0.13747665\n",
      "Iteration 341, loss = 0.13726833\n",
      "Iteration 342, loss = 0.13632238\n",
      "Iteration 343, loss = 0.13676676\n",
      "Iteration 344, loss = 0.13563478\n",
      "Iteration 345, loss = 0.13447565\n",
      "Iteration 346, loss = 0.13342548\n",
      "Iteration 347, loss = 0.13279199\n",
      "Iteration 348, loss = 0.13279425\n",
      "Iteration 349, loss = 0.13218100\n",
      "Iteration 350, loss = 0.13098079\n",
      "Iteration 351, loss = 0.13057670\n",
      "Iteration 352, loss = 0.12963065\n",
      "Iteration 353, loss = 0.13001694\n",
      "Iteration 354, loss = 0.12812116\n",
      "Iteration 355, loss = 0.12748548\n",
      "Iteration 356, loss = 0.12694120\n",
      "Iteration 357, loss = 0.12665763\n",
      "Iteration 358, loss = 0.12579680\n",
      "Iteration 359, loss = 0.12516841\n",
      "Iteration 360, loss = 0.12510071\n",
      "Iteration 361, loss = 0.12477417\n",
      "Iteration 362, loss = 0.12341135\n",
      "Iteration 363, loss = 0.12300864\n",
      "Iteration 364, loss = 0.12248181\n",
      "Iteration 365, loss = 0.12208705\n",
      "Iteration 366, loss = 0.12153795\n",
      "Iteration 367, loss = 0.12084930\n",
      "Iteration 368, loss = 0.12027260\n",
      "Iteration 369, loss = 0.11969669\n",
      "Iteration 370, loss = 0.12087381\n",
      "Iteration 371, loss = 0.11925128\n",
      "Iteration 372, loss = 0.11817006\n",
      "Iteration 373, loss = 0.11781213\n",
      "Iteration 374, loss = 0.11739511\n",
      "Iteration 375, loss = 0.11681189\n",
      "Iteration 376, loss = 0.11731025\n",
      "Iteration 377, loss = 0.11588430\n",
      "Iteration 378, loss = 0.11550788\n",
      "Iteration 379, loss = 0.11546138\n",
      "Iteration 380, loss = 0.11455179\n",
      "Iteration 381, loss = 0.11449818\n",
      "Iteration 382, loss = 0.11434078\n",
      "Iteration 383, loss = 0.11383834\n",
      "Iteration 384, loss = 0.11365960\n",
      "Iteration 385, loss = 0.11297264\n",
      "Iteration 386, loss = 0.11398252\n",
      "Iteration 387, loss = 0.11097893\n",
      "Iteration 388, loss = 0.11201396\n",
      "Iteration 389, loss = 0.11038972\n",
      "Iteration 390, loss = 0.11019370\n",
      "Iteration 391, loss = 0.11083907\n",
      "Iteration 392, loss = 0.10928082\n",
      "Iteration 393, loss = 0.10899131\n",
      "Iteration 394, loss = 0.10907017\n",
      "Iteration 395, loss = 0.10797213\n",
      "Iteration 396, loss = 0.10797780\n",
      "Iteration 397, loss = 0.10742001\n",
      "Iteration 398, loss = 0.10699461\n",
      "Iteration 399, loss = 0.10677029\n",
      "Iteration 400, loss = 0.10605722\n",
      "Iteration 401, loss = 0.10630576\n",
      "Iteration 402, loss = 0.10564104\n",
      "Iteration 403, loss = 0.10470326\n",
      "Iteration 404, loss = 0.10498820\n",
      "Iteration 405, loss = 0.10468928\n",
      "Iteration 406, loss = 0.10391193\n",
      "Iteration 407, loss = 0.10366376\n",
      "Iteration 408, loss = 0.10348182\n",
      "Iteration 409, loss = 0.10347765\n",
      "Iteration 410, loss = 0.10248872\n",
      "Iteration 411, loss = 0.10323755\n",
      "Iteration 412, loss = 0.10169646\n",
      "Iteration 413, loss = 0.10199538\n",
      "Iteration 414, loss = 0.10103232\n",
      "Iteration 415, loss = 0.10068222\n",
      "Iteration 416, loss = 0.10108710\n",
      "Iteration 417, loss = 0.10030901\n",
      "Iteration 418, loss = 0.10020884\n",
      "Iteration 419, loss = 0.10068700\n",
      "Iteration 420, loss = 0.09922021\n",
      "Iteration 421, loss = 0.09954624\n",
      "Iteration 422, loss = 0.09941210\n",
      "Iteration 423, loss = 0.09848657\n",
      "Iteration 424, loss = 0.09820775\n",
      "Iteration 425, loss = 0.09798039\n",
      "Iteration 426, loss = 0.09765758\n",
      "Iteration 427, loss = 0.09731151\n",
      "Iteration 428, loss = 0.09684018\n",
      "Iteration 429, loss = 0.09673163\n",
      "Iteration 430, loss = 0.09607905\n",
      "Iteration 431, loss = 0.09606133\n",
      "Iteration 432, loss = 0.09614069\n",
      "Iteration 433, loss = 0.09551930\n",
      "Iteration 434, loss = 0.09544014\n",
      "Iteration 435, loss = 0.09555658\n",
      "Iteration 436, loss = 0.09469890\n",
      "Iteration 437, loss = 0.09423732\n",
      "Iteration 438, loss = 0.09410912\n",
      "Iteration 439, loss = 0.09373566\n",
      "Iteration 440, loss = 0.09448603\n",
      "Iteration 441, loss = 0.09361725\n",
      "Iteration 442, loss = 0.09350033\n",
      "Iteration 443, loss = 0.09384023\n",
      "Iteration 444, loss = 0.09255691\n",
      "Iteration 445, loss = 0.09248989\n",
      "Iteration 446, loss = 0.09205351\n",
      "Iteration 447, loss = 0.09186275\n",
      "Iteration 448, loss = 0.09145789\n",
      "Iteration 449, loss = 0.09232045\n",
      "Iteration 450, loss = 0.09146642\n",
      "Iteration 451, loss = 0.09082700\n",
      "Iteration 452, loss = 0.09058789\n",
      "Iteration 453, loss = 0.09034887\n",
      "Iteration 454, loss = 0.09105675\n",
      "Iteration 455, loss = 0.08971537\n",
      "Iteration 456, loss = 0.08989323\n",
      "Iteration 457, loss = 0.09013146\n",
      "Iteration 458, loss = 0.08975975\n",
      "Iteration 459, loss = 0.08876301\n",
      "Iteration 460, loss = 0.08879017\n",
      "Iteration 461, loss = 0.08909395\n",
      "Iteration 462, loss = 0.08850167\n",
      "Iteration 463, loss = 0.08794613\n",
      "Iteration 464, loss = 0.08799311\n",
      "Iteration 465, loss = 0.08768993\n",
      "Iteration 466, loss = 0.08796294\n",
      "Iteration 467, loss = 0.08902575\n",
      "Iteration 468, loss = 0.08748767\n",
      "Iteration 469, loss = 0.08735916\n",
      "Iteration 470, loss = 0.08709167\n",
      "Iteration 471, loss = 0.08631843\n",
      "Iteration 472, loss = 0.08624328\n",
      "Iteration 473, loss = 0.08607356\n",
      "Iteration 474, loss = 0.08596251\n",
      "Iteration 475, loss = 0.08635505\n",
      "Iteration 476, loss = 0.08642063\n",
      "Iteration 477, loss = 0.08530837\n",
      "Iteration 478, loss = 0.08617291\n",
      "Iteration 479, loss = 0.08508666\n",
      "Iteration 480, loss = 0.08469560\n",
      "Iteration 481, loss = 0.08534564\n",
      "Iteration 482, loss = 0.08462598\n",
      "Iteration 483, loss = 0.08425728\n",
      "Iteration 484, loss = 0.08535907\n",
      "Iteration 485, loss = 0.08357925\n",
      "Iteration 486, loss = 0.08383392\n",
      "Iteration 487, loss = 0.08348297\n",
      "Iteration 488, loss = 0.08375744\n",
      "Iteration 489, loss = 0.08399163\n",
      "Iteration 490, loss = 0.08309759\n",
      "Iteration 491, loss = 0.08299841\n",
      "Iteration 492, loss = 0.08282044\n",
      "Iteration 493, loss = 0.08242821\n",
      "Iteration 494, loss = 0.08274709\n",
      "Iteration 495, loss = 0.08378272\n",
      "Iteration 496, loss = 0.08344389\n",
      "Iteration 497, loss = 0.08231418\n",
      "Iteration 498, loss = 0.08168701\n",
      "Iteration 499, loss = 0.08154903\n",
      "Iteration 500, loss = 0.08125917\n",
      "Iteration 501, loss = 0.08127638\n",
      "Iteration 502, loss = 0.08150431\n",
      "Iteration 503, loss = 0.08094008\n",
      "Iteration 504, loss = 0.08059724\n",
      "Iteration 505, loss = 0.08066410\n",
      "Iteration 506, loss = 0.08048403\n",
      "Iteration 507, loss = 0.08059519\n",
      "Iteration 508, loss = 0.08064955\n",
      "Iteration 509, loss = 0.08221156\n",
      "Iteration 510, loss = 0.08037903\n",
      "Iteration 511, loss = 0.08009263\n",
      "Iteration 512, loss = 0.07955607\n",
      "Iteration 513, loss = 0.07999439\n",
      "Iteration 514, loss = 0.07948876\n",
      "Iteration 515, loss = 0.08040284\n",
      "Iteration 516, loss = 0.07918435\n",
      "Iteration 517, loss = 0.07926106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 518, loss = 0.07874123\n",
      "Iteration 519, loss = 0.07863365\n",
      "Iteration 520, loss = 0.08039820\n",
      "Iteration 521, loss = 0.07875221\n",
      "Iteration 522, loss = 0.07974549\n",
      "Iteration 523, loss = 0.07826863\n",
      "Iteration 524, loss = 0.07901365\n",
      "Iteration 525, loss = 0.07805797\n",
      "Iteration 526, loss = 0.07798271\n",
      "Iteration 527, loss = 0.07755281\n",
      "Iteration 528, loss = 0.07775579\n",
      "Iteration 529, loss = 0.07843010\n",
      "Iteration 530, loss = 0.07705730\n",
      "Iteration 531, loss = 0.07895917\n",
      "Iteration 532, loss = 0.07712772\n",
      "Iteration 533, loss = 0.07756381\n",
      "Iteration 534, loss = 0.07701730\n",
      "Iteration 535, loss = 0.07700285\n",
      "Iteration 536, loss = 0.07636367\n",
      "Iteration 537, loss = 0.07676999\n",
      "Iteration 538, loss = 0.07665265\n",
      "Iteration 539, loss = 0.07646093\n",
      "Iteration 540, loss = 0.07662658\n",
      "Iteration 541, loss = 0.07631941\n",
      "Iteration 542, loss = 0.07589661\n",
      "Iteration 543, loss = 0.07686601\n",
      "Iteration 544, loss = 0.07626118\n",
      "Iteration 545, loss = 0.07534749\n",
      "Iteration 546, loss = 0.07652611\n",
      "Iteration 547, loss = 0.07592814\n",
      "Iteration 548, loss = 0.07511804\n",
      "Iteration 549, loss = 0.07517617\n",
      "Iteration 550, loss = 0.07493200\n",
      "Iteration 551, loss = 0.07550859\n",
      "Iteration 552, loss = 0.07512360\n",
      "Iteration 553, loss = 0.07555581\n",
      "Iteration 554, loss = 0.07457377\n",
      "Iteration 555, loss = 0.07428449\n",
      "Iteration 556, loss = 0.07430811\n",
      "Iteration 557, loss = 0.07432230\n",
      "Iteration 558, loss = 0.07406934\n",
      "Iteration 559, loss = 0.07449408\n",
      "Iteration 560, loss = 0.07388756\n",
      "Iteration 561, loss = 0.07471264\n",
      "Iteration 562, loss = 0.07413824\n",
      "Iteration 563, loss = 0.07350118\n",
      "Iteration 564, loss = 0.07368797\n",
      "Iteration 565, loss = 0.07365995\n",
      "Iteration 566, loss = 0.07339921\n",
      "Iteration 567, loss = 0.07322031\n",
      "Iteration 568, loss = 0.07336052\n",
      "Iteration 569, loss = 0.07351394\n",
      "Iteration 570, loss = 0.07292813\n",
      "Iteration 571, loss = 0.07450841\n",
      "Iteration 572, loss = 0.07397343\n",
      "Iteration 573, loss = 0.07380928\n",
      "Iteration 574, loss = 0.07254575\n",
      "Iteration 575, loss = 0.07387179\n",
      "Iteration 576, loss = 0.07554403\n",
      "Iteration 577, loss = 0.07365857\n",
      "Iteration 578, loss = 0.07279916\n",
      "Iteration 579, loss = 0.07209456\n",
      "Iteration 580, loss = 0.07209991\n",
      "Iteration 581, loss = 0.07220480\n",
      "Iteration 582, loss = 0.07173207\n",
      "Iteration 583, loss = 0.07226049\n",
      "Iteration 584, loss = 0.07223156\n",
      "Iteration 585, loss = 0.07161032\n",
      "Iteration 586, loss = 0.07180289\n",
      "Iteration 587, loss = 0.07148428\n",
      "Iteration 588, loss = 0.07156222\n",
      "Iteration 589, loss = 0.07184570\n",
      "Iteration 590, loss = 0.07154628\n",
      "Iteration 591, loss = 0.07157445\n",
      "Iteration 592, loss = 0.07104759\n",
      "Iteration 593, loss = 0.07140649\n",
      "Iteration 594, loss = 0.07188242\n",
      "Iteration 595, loss = 0.07242674\n",
      "Iteration 596, loss = 0.07122241\n",
      "Iteration 597, loss = 0.07038212\n",
      "Iteration 598, loss = 0.07085087\n",
      "Iteration 599, loss = 0.07062203\n",
      "Iteration 600, loss = 0.07076138\n",
      "Iteration 601, loss = 0.07013371\n",
      "Iteration 602, loss = 0.07156134\n",
      "Iteration 603, loss = 0.07035542\n",
      "Iteration 604, loss = 0.07027510\n",
      "Iteration 605, loss = 0.07008959\n",
      "Iteration 606, loss = 0.07073299\n",
      "Iteration 607, loss = 0.07006591\n",
      "Iteration 608, loss = 0.07026755\n",
      "Iteration 609, loss = 0.06995736\n",
      "Iteration 610, loss = 0.07015856\n",
      "Iteration 611, loss = 0.07094316\n",
      "Iteration 612, loss = 0.07085794\n",
      "Iteration 613, loss = 0.07045280\n",
      "Iteration 614, loss = 0.07025822\n",
      "Iteration 615, loss = 0.06996655\n",
      "Iteration 616, loss = 0.06961770\n",
      "Iteration 617, loss = 0.06944138\n",
      "Iteration 618, loss = 0.06913816\n",
      "Iteration 619, loss = 0.06944658\n",
      "Iteration 620, loss = 0.06924027\n",
      "Iteration 621, loss = 0.06923464\n",
      "Iteration 622, loss = 0.06981291\n",
      "Iteration 623, loss = 0.06886609\n",
      "Iteration 624, loss = 0.06864834\n",
      "Iteration 625, loss = 0.06928274\n",
      "Iteration 626, loss = 0.06867407\n",
      "Iteration 627, loss = 0.06923544\n",
      "Iteration 628, loss = 0.06866149\n",
      "Iteration 629, loss = 0.06910938\n",
      "Iteration 630, loss = 0.06974255\n",
      "Iteration 631, loss = 0.06997433\n",
      "Iteration 632, loss = 0.06856331\n",
      "Iteration 633, loss = 0.06856085\n",
      "Iteration 634, loss = 0.06862475\n",
      "Iteration 635, loss = 0.06847608\n",
      "Iteration 636, loss = 0.06853119\n",
      "Iteration 637, loss = 0.06847622\n",
      "Iteration 638, loss = 0.06793077\n",
      "Iteration 639, loss = 0.06811784\n",
      "Iteration 640, loss = 0.06917755\n",
      "Iteration 641, loss = 0.06828441\n",
      "Iteration 642, loss = 0.06785074\n",
      "Iteration 643, loss = 0.06878787\n",
      "Iteration 644, loss = 0.06813629\n",
      "Iteration 645, loss = 0.06766626\n",
      "Iteration 646, loss = 0.06743552\n",
      "Iteration 647, loss = 0.06799343\n",
      "Iteration 648, loss = 0.06739635\n",
      "Iteration 649, loss = 0.06742987\n",
      "Iteration 650, loss = 0.06727568\n",
      "Iteration 651, loss = 0.06735244\n",
      "Iteration 652, loss = 0.06890752\n",
      "Iteration 653, loss = 0.06749004\n",
      "Iteration 654, loss = 0.06716531\n",
      "Iteration 655, loss = 0.06738067\n",
      "Iteration 656, loss = 0.06695755\n",
      "Iteration 657, loss = 0.06673366\n",
      "Iteration 658, loss = 0.06680409\n",
      "Iteration 659, loss = 0.06675809\n",
      "Iteration 660, loss = 0.06687117\n",
      "Iteration 661, loss = 0.06700169\n",
      "Iteration 662, loss = 0.06670694\n",
      "Iteration 663, loss = 0.06696522\n",
      "Iteration 664, loss = 0.06651872\n",
      "Iteration 665, loss = 0.06733515\n",
      "Iteration 666, loss = 0.06732717\n",
      "Iteration 667, loss = 0.06668581\n",
      "Iteration 668, loss = 0.06688455\n",
      "Iteration 669, loss = 0.06620821\n",
      "Iteration 670, loss = 0.06607947\n",
      "Iteration 671, loss = 0.06625511\n",
      "Iteration 672, loss = 0.06683022\n",
      "Iteration 673, loss = 0.06667855\n",
      "Iteration 674, loss = 0.06583608\n",
      "Iteration 675, loss = 0.06628329\n",
      "Iteration 676, loss = 0.06605465\n",
      "Iteration 677, loss = 0.06600243\n",
      "Iteration 678, loss = 0.06571738\n",
      "Iteration 679, loss = 0.06646695\n",
      "Iteration 680, loss = 0.06640064\n",
      "Iteration 681, loss = 0.06619857\n",
      "Iteration 682, loss = 0.06569202\n",
      "Iteration 683, loss = 0.06607133\n",
      "Iteration 684, loss = 0.06670168\n",
      "Iteration 685, loss = 0.06552655\n",
      "Iteration 686, loss = 0.06544556\n",
      "Iteration 687, loss = 0.06533544\n",
      "Iteration 688, loss = 0.06549421\n",
      "Iteration 689, loss = 0.06558830\n",
      "Iteration 690, loss = 0.06527396\n",
      "Iteration 691, loss = 0.06566649\n",
      "Iteration 692, loss = 0.06536234\n",
      "Iteration 693, loss = 0.06524709\n",
      "Iteration 694, loss = 0.06525080\n",
      "Iteration 695, loss = 0.06685819\n",
      "Iteration 696, loss = 0.06471028\n",
      "Iteration 697, loss = 0.06613614\n",
      "Iteration 698, loss = 0.06555661\n",
      "Iteration 699, loss = 0.06513446\n",
      "Iteration 700, loss = 0.06463616\n",
      "Iteration 701, loss = 0.06512560\n",
      "Iteration 702, loss = 0.06831334\n",
      "Iteration 703, loss = 0.06539158\n",
      "Iteration 704, loss = 0.06607799\n",
      "Iteration 705, loss = 0.06480952\n",
      "Iteration 706, loss = 0.06559176\n",
      "Iteration 707, loss = 0.06467617\n",
      "Iteration 708, loss = 0.06499282\n",
      "Iteration 709, loss = 0.06662775\n",
      "Iteration 710, loss = 0.06507410\n",
      "Iteration 711, loss = 0.06519360\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', batch_size=10, hidden_layer_sizes=(4, 5),\n",
       "              max_iter=2000, tol=1e-06, verbose=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x141adf6c3d0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(network.loss_curve_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = network.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(cm, cmap=\"Blues\", fmt=\"d\", annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Specify instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 4), (150, 4))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = x_test[0].reshape(1,-1)\n",
    "instance.shape,inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.predict(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foe regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()\n",
    "x = boston.data\n",
    "y = boston.target\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "nr = MinMaxScaler()\n",
    "x_train = nr.fit_transform(x_train)\n",
    "x_test = nr.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 296.04059598\n",
      "Iteration 2, loss = 295.20064536\n",
      "Iteration 3, loss = 294.37920883\n",
      "Iteration 4, loss = 293.55040512\n",
      "Iteration 5, loss = 292.73032275\n",
      "Iteration 6, loss = 291.91603274\n",
      "Iteration 7, loss = 291.09851481\n",
      "Iteration 8, loss = 290.28636783\n",
      "Iteration 9, loss = 289.47266338\n",
      "Iteration 10, loss = 288.66694164\n",
      "Iteration 11, loss = 287.84853457\n",
      "Iteration 12, loss = 287.02741585\n",
      "Iteration 13, loss = 286.19437502\n",
      "Iteration 14, loss = 285.35866290\n",
      "Iteration 15, loss = 284.52114475\n",
      "Iteration 16, loss = 283.69570887\n",
      "Iteration 17, loss = 282.85482564\n",
      "Iteration 18, loss = 281.98596415\n",
      "Iteration 19, loss = 281.07593240\n",
      "Iteration 20, loss = 280.14689463\n",
      "Iteration 21, loss = 279.19142169\n",
      "Iteration 22, loss = 278.20028050\n",
      "Iteration 23, loss = 277.17299629\n",
      "Iteration 24, loss = 276.14806597\n",
      "Iteration 25, loss = 275.12547958\n",
      "Iteration 26, loss = 274.07929135\n",
      "Iteration 27, loss = 273.01493068\n",
      "Iteration 28, loss = 271.92415213\n",
      "Iteration 29, loss = 270.81868272\n",
      "Iteration 30, loss = 269.68314402\n",
      "Iteration 31, loss = 268.51530196\n",
      "Iteration 32, loss = 267.33862046\n",
      "Iteration 33, loss = 266.15288263\n",
      "Iteration 34, loss = 264.96271212\n",
      "Iteration 35, loss = 263.76308717\n",
      "Iteration 36, loss = 262.57287553\n",
      "Iteration 37, loss = 261.38951435\n",
      "Iteration 38, loss = 260.20010834\n",
      "Iteration 39, loss = 259.02396305\n",
      "Iteration 40, loss = 257.85051754\n",
      "Iteration 41, loss = 256.66819842\n",
      "Iteration 42, loss = 255.48914625\n",
      "Iteration 43, loss = 254.31941120\n",
      "Iteration 44, loss = 253.17198268\n",
      "Iteration 45, loss = 252.04943692\n",
      "Iteration 46, loss = 250.93527659\n",
      "Iteration 47, loss = 249.81318757\n",
      "Iteration 48, loss = 248.66430035\n",
      "Iteration 49, loss = 247.51728633\n",
      "Iteration 50, loss = 246.35186764\n",
      "Iteration 51, loss = 245.21273203\n",
      "Iteration 52, loss = 244.08531139\n",
      "Iteration 53, loss = 242.97730489\n",
      "Iteration 54, loss = 241.88763007\n",
      "Iteration 55, loss = 240.77086726\n",
      "Iteration 56, loss = 239.66417648\n",
      "Iteration 57, loss = 238.55207585\n",
      "Iteration 58, loss = 237.44816939\n",
      "Iteration 59, loss = 236.34393456\n",
      "Iteration 60, loss = 235.23196090\n",
      "Iteration 61, loss = 234.12293183\n",
      "Iteration 62, loss = 232.99415738\n",
      "Iteration 63, loss = 231.87353755\n",
      "Iteration 64, loss = 230.75682277\n",
      "Iteration 65, loss = 229.63217801\n",
      "Iteration 66, loss = 228.50104399\n",
      "Iteration 67, loss = 227.33995264\n",
      "Iteration 68, loss = 226.17920737\n",
      "Iteration 69, loss = 225.01255862\n",
      "Iteration 70, loss = 223.83804676\n",
      "Iteration 71, loss = 222.67397064\n",
      "Iteration 72, loss = 221.51962319\n",
      "Iteration 73, loss = 220.36311185\n",
      "Iteration 74, loss = 219.21432249\n",
      "Iteration 75, loss = 218.05799271\n",
      "Iteration 76, loss = 216.89842001\n",
      "Iteration 77, loss = 215.73812594\n",
      "Iteration 78, loss = 214.57594317\n",
      "Iteration 79, loss = 213.41481464\n",
      "Iteration 80, loss = 212.24204499\n",
      "Iteration 81, loss = 211.05584349\n",
      "Iteration 82, loss = 209.84909888\n",
      "Iteration 83, loss = 208.64615593\n",
      "Iteration 84, loss = 207.43710331\n",
      "Iteration 85, loss = 206.21677169\n",
      "Iteration 86, loss = 204.98706312\n",
      "Iteration 87, loss = 203.74727485\n",
      "Iteration 88, loss = 202.46262974\n",
      "Iteration 89, loss = 201.21223288\n",
      "Iteration 90, loss = 199.93598934\n",
      "Iteration 91, loss = 198.67517306\n",
      "Iteration 92, loss = 197.41492193\n",
      "Iteration 93, loss = 196.15125629\n",
      "Iteration 94, loss = 194.90646407\n",
      "Iteration 95, loss = 193.64492855\n",
      "Iteration 96, loss = 192.36866041\n",
      "Iteration 97, loss = 191.10042813\n",
      "Iteration 98, loss = 189.85281019\n",
      "Iteration 99, loss = 188.61982519\n",
      "Iteration 100, loss = 187.38670324\n",
      "Iteration 101, loss = 186.12609704\n",
      "Iteration 102, loss = 184.81313744\n",
      "Iteration 103, loss = 183.49900237\n",
      "Iteration 104, loss = 182.19894603\n",
      "Iteration 105, loss = 180.90127381\n",
      "Iteration 106, loss = 179.64471217\n",
      "Iteration 107, loss = 178.42404909\n",
      "Iteration 108, loss = 177.25105661\n",
      "Iteration 109, loss = 176.06860397\n",
      "Iteration 110, loss = 174.87510607\n",
      "Iteration 111, loss = 173.68151223\n",
      "Iteration 112, loss = 172.47557122\n",
      "Iteration 113, loss = 171.23401616\n",
      "Iteration 114, loss = 169.93710125\n",
      "Iteration 115, loss = 168.61927774\n",
      "Iteration 116, loss = 167.27855793\n",
      "Iteration 117, loss = 165.96514375\n",
      "Iteration 118, loss = 164.67243331\n",
      "Iteration 119, loss = 163.39912394\n",
      "Iteration 120, loss = 162.11354901\n",
      "Iteration 121, loss = 160.82625047\n",
      "Iteration 122, loss = 159.59642811\n",
      "Iteration 123, loss = 158.42232356\n",
      "Iteration 124, loss = 157.24713235\n",
      "Iteration 125, loss = 156.07268778\n",
      "Iteration 126, loss = 154.87195232\n",
      "Iteration 127, loss = 153.65588056\n",
      "Iteration 128, loss = 152.43886463\n",
      "Iteration 129, loss = 151.22484854\n",
      "Iteration 130, loss = 150.00462644\n",
      "Iteration 131, loss = 148.77381849\n",
      "Iteration 132, loss = 147.55655330\n",
      "Iteration 133, loss = 146.36093735\n",
      "Iteration 134, loss = 145.14938142\n",
      "Iteration 135, loss = 143.94288877\n",
      "Iteration 136, loss = 142.75020435\n",
      "Iteration 137, loss = 141.58917496\n",
      "Iteration 138, loss = 140.42845503\n",
      "Iteration 139, loss = 139.28298813\n",
      "Iteration 140, loss = 138.13406473\n",
      "Iteration 141, loss = 137.00409737\n",
      "Iteration 142, loss = 135.90499678\n",
      "Iteration 143, loss = 134.79254427\n",
      "Iteration 144, loss = 133.67014936\n",
      "Iteration 145, loss = 132.60413989\n",
      "Iteration 146, loss = 131.48638190\n",
      "Iteration 147, loss = 130.38953363\n",
      "Iteration 148, loss = 129.29641880\n",
      "Iteration 149, loss = 128.24104596\n",
      "Iteration 150, loss = 127.15867200\n",
      "Iteration 151, loss = 126.10776044\n",
      "Iteration 152, loss = 125.06216292\n",
      "Iteration 153, loss = 124.01969214\n",
      "Iteration 154, loss = 122.99821335\n",
      "Iteration 155, loss = 121.95605793\n",
      "Iteration 156, loss = 120.94731036\n",
      "Iteration 157, loss = 119.99531673\n",
      "Iteration 158, loss = 119.03620210\n",
      "Iteration 159, loss = 118.08097764\n",
      "Iteration 160, loss = 117.11997133\n",
      "Iteration 161, loss = 116.14938902\n",
      "Iteration 162, loss = 115.17984096\n",
      "Iteration 163, loss = 114.24785636\n",
      "Iteration 164, loss = 113.30247396\n",
      "Iteration 165, loss = 112.35226121\n",
      "Iteration 166, loss = 111.41321743\n",
      "Iteration 167, loss = 110.51139265\n",
      "Iteration 168, loss = 109.67721576\n",
      "Iteration 169, loss = 108.88081115\n",
      "Iteration 170, loss = 108.08452442\n",
      "Iteration 171, loss = 107.31931436\n",
      "Iteration 172, loss = 106.55355721\n",
      "Iteration 173, loss = 105.80651621\n",
      "Iteration 174, loss = 105.01921321\n",
      "Iteration 175, loss = 104.22950704\n",
      "Iteration 176, loss = 103.43591141\n",
      "Iteration 177, loss = 102.66309959\n",
      "Iteration 178, loss = 101.91312728\n",
      "Iteration 179, loss = 101.14783432\n",
      "Iteration 180, loss = 100.41465334\n",
      "Iteration 181, loss = 99.68934176\n",
      "Iteration 182, loss = 98.96918924\n",
      "Iteration 183, loss = 98.25895337\n",
      "Iteration 184, loss = 97.53998132\n",
      "Iteration 185, loss = 96.79320686\n",
      "Iteration 186, loss = 96.05972909\n",
      "Iteration 187, loss = 95.37624986\n",
      "Iteration 188, loss = 94.71225129\n",
      "Iteration 189, loss = 94.07596219\n",
      "Iteration 190, loss = 93.42309732\n",
      "Iteration 191, loss = 92.79440544\n",
      "Iteration 192, loss = 92.12812233\n",
      "Iteration 193, loss = 91.45452434\n",
      "Iteration 194, loss = 90.79160550\n",
      "Iteration 195, loss = 90.14767399\n",
      "Iteration 196, loss = 89.53504544\n",
      "Iteration 197, loss = 88.94281182\n",
      "Iteration 198, loss = 88.36699498\n",
      "Iteration 199, loss = 87.77247977\n",
      "Iteration 200, loss = 87.12750649\n",
      "Iteration 201, loss = 86.51172886\n",
      "Iteration 202, loss = 85.92648976\n",
      "Iteration 203, loss = 85.33462096\n",
      "Iteration 204, loss = 84.75755037\n",
      "Iteration 205, loss = 84.18496951\n",
      "Iteration 206, loss = 83.65783640\n",
      "Iteration 207, loss = 83.12309788\n",
      "Iteration 208, loss = 82.62335612\n",
      "Iteration 209, loss = 82.11715057\n",
      "Iteration 210, loss = 81.61757611\n",
      "Iteration 211, loss = 81.11750769\n",
      "Iteration 212, loss = 80.55926013\n",
      "Iteration 213, loss = 80.02934575\n",
      "Iteration 214, loss = 79.51105651\n",
      "Iteration 215, loss = 78.99266105\n",
      "Iteration 216, loss = 78.46727957\n",
      "Iteration 217, loss = 77.95777267\n",
      "Iteration 218, loss = 77.47318024\n",
      "Iteration 219, loss = 76.97730838\n",
      "Iteration 220, loss = 76.48638724\n",
      "Iteration 221, loss = 76.03160863\n",
      "Iteration 222, loss = 75.61310426\n",
      "Iteration 223, loss = 75.21585855\n",
      "Iteration 224, loss = 74.79861180\n",
      "Iteration 225, loss = 74.36690916\n",
      "Iteration 226, loss = 73.98546900\n",
      "Iteration 227, loss = 73.63559419\n",
      "Iteration 228, loss = 73.29667353\n",
      "Iteration 229, loss = 72.91840620\n",
      "Iteration 230, loss = 72.57480811\n",
      "Iteration 231, loss = 72.26678743\n",
      "Iteration 232, loss = 71.94787733\n",
      "Iteration 233, loss = 71.62508863\n",
      "Iteration 234, loss = 71.34533204\n",
      "Iteration 235, loss = 71.08673730\n",
      "Iteration 236, loss = 70.79428207\n",
      "Iteration 237, loss = 70.49715925\n",
      "Iteration 238, loss = 70.18679527\n",
      "Iteration 239, loss = 69.88047603\n",
      "Iteration 240, loss = 69.59370546\n",
      "Iteration 241, loss = 69.29764722\n",
      "Iteration 242, loss = 69.03916431\n",
      "Iteration 243, loss = 68.77181092\n",
      "Iteration 244, loss = 68.49744052\n",
      "Iteration 245, loss = 68.23087805\n",
      "Iteration 246, loss = 67.98316872\n",
      "Iteration 247, loss = 67.75498254\n",
      "Iteration 248, loss = 67.52034354\n",
      "Iteration 249, loss = 67.24815987\n",
      "Iteration 250, loss = 67.00212741\n",
      "Iteration 251, loss = 66.76898634\n",
      "Iteration 252, loss = 66.55748683\n",
      "Iteration 253, loss = 66.33874524\n",
      "Iteration 254, loss = 66.11238405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 255, loss = 65.89538022\n",
      "Iteration 256, loss = 65.68110848\n",
      "Iteration 257, loss = 65.47781390\n",
      "Iteration 258, loss = 65.25945980\n",
      "Iteration 259, loss = 65.01310544\n",
      "Iteration 260, loss = 64.79023081\n",
      "Iteration 261, loss = 64.57776657\n",
      "Iteration 262, loss = 64.36500257\n",
      "Iteration 263, loss = 64.17442358\n",
      "Iteration 264, loss = 63.97960285\n",
      "Iteration 265, loss = 63.74797324\n",
      "Iteration 266, loss = 63.52191192\n",
      "Iteration 267, loss = 63.29433610\n",
      "Iteration 268, loss = 63.06382108\n",
      "Iteration 269, loss = 62.83678124\n",
      "Iteration 270, loss = 62.64975296\n",
      "Iteration 271, loss = 62.45956273\n",
      "Iteration 272, loss = 62.27841287\n",
      "Iteration 273, loss = 62.10747103\n",
      "Iteration 274, loss = 61.94216944\n",
      "Iteration 275, loss = 61.76209432\n",
      "Iteration 276, loss = 61.58672735\n",
      "Iteration 277, loss = 61.38845731\n",
      "Iteration 278, loss = 61.18906819\n",
      "Iteration 279, loss = 61.00502142\n",
      "Iteration 280, loss = 60.83132417\n",
      "Iteration 281, loss = 60.66025087\n",
      "Iteration 282, loss = 60.50913915\n",
      "Iteration 283, loss = 60.37507598\n",
      "Iteration 284, loss = 60.24076009\n",
      "Iteration 285, loss = 60.10726312\n",
      "Iteration 286, loss = 59.97739025\n",
      "Iteration 287, loss = 59.85746867\n",
      "Iteration 288, loss = 59.73909824\n",
      "Iteration 289, loss = 59.62298733\n",
      "Iteration 290, loss = 59.48744331\n",
      "Iteration 291, loss = 59.35966485\n",
      "Iteration 292, loss = 59.22107215\n",
      "Iteration 293, loss = 59.08394834\n",
      "Iteration 294, loss = 58.94185783\n",
      "Iteration 295, loss = 58.80022751\n",
      "Iteration 296, loss = 58.65357918\n",
      "Iteration 297, loss = 58.49270439\n",
      "Iteration 298, loss = 58.32503295\n",
      "Iteration 299, loss = 58.16581475\n",
      "Iteration 300, loss = 58.01205570\n",
      "Iteration 301, loss = 57.87106170\n",
      "Iteration 302, loss = 57.74041908\n",
      "Iteration 303, loss = 57.62027140\n",
      "Iteration 304, loss = 57.49295209\n",
      "Iteration 305, loss = 57.37407830\n",
      "Iteration 306, loss = 57.25401103\n",
      "Iteration 307, loss = 57.14032934\n",
      "Iteration 308, loss = 57.03536016\n",
      "Iteration 309, loss = 56.92614075\n",
      "Iteration 310, loss = 56.81755322\n",
      "Iteration 311, loss = 56.71372394\n",
      "Iteration 312, loss = 56.60896502\n",
      "Iteration 313, loss = 56.49932024\n",
      "Iteration 314, loss = 56.38185886\n",
      "Iteration 315, loss = 56.26200121\n",
      "Iteration 316, loss = 56.13816068\n",
      "Iteration 317, loss = 56.01476373\n",
      "Iteration 318, loss = 55.89163355\n",
      "Iteration 319, loss = 55.76341510\n",
      "Iteration 320, loss = 55.63827846\n",
      "Iteration 321, loss = 55.50955687\n",
      "Iteration 322, loss = 55.39677243\n",
      "Iteration 323, loss = 55.28419030\n",
      "Iteration 324, loss = 55.17357552\n",
      "Iteration 325, loss = 55.06239922\n",
      "Iteration 326, loss = 54.94548797\n",
      "Iteration 327, loss = 54.83228618\n",
      "Iteration 328, loss = 54.72627075\n",
      "Iteration 329, loss = 54.59960819\n",
      "Iteration 330, loss = 54.47157428\n",
      "Iteration 331, loss = 54.35473534\n",
      "Iteration 332, loss = 54.22631534\n",
      "Iteration 333, loss = 54.10086568\n",
      "Iteration 334, loss = 53.97069939\n",
      "Iteration 335, loss = 53.84330571\n",
      "Iteration 336, loss = 53.71545350\n",
      "Iteration 337, loss = 53.58729859\n",
      "Iteration 338, loss = 53.46602060\n",
      "Iteration 339, loss = 53.34565745\n",
      "Iteration 340, loss = 53.23482373\n",
      "Iteration 341, loss = 53.12489994\n",
      "Iteration 342, loss = 53.02113914\n",
      "Iteration 343, loss = 52.91675521\n",
      "Iteration 344, loss = 52.79633492\n",
      "Iteration 345, loss = 52.68319399\n",
      "Iteration 346, loss = 52.56475757\n",
      "Iteration 347, loss = 52.46961305\n",
      "Iteration 348, loss = 52.34894665\n",
      "Iteration 349, loss = 52.23672035\n",
      "Iteration 350, loss = 52.13446201\n",
      "Iteration 351, loss = 52.02691038\n",
      "Iteration 352, loss = 51.90336917\n",
      "Iteration 353, loss = 51.76734026\n",
      "Iteration 354, loss = 51.64257624\n",
      "Iteration 355, loss = 51.52179203\n",
      "Iteration 356, loss = 51.41825942\n",
      "Iteration 357, loss = 51.31082178\n",
      "Iteration 358, loss = 51.19455180\n",
      "Iteration 359, loss = 51.07411554\n",
      "Iteration 360, loss = 50.95997312\n",
      "Iteration 361, loss = 50.83764091\n",
      "Iteration 362, loss = 50.71897793\n",
      "Iteration 363, loss = 50.59741413\n",
      "Iteration 364, loss = 50.47874868\n",
      "Iteration 365, loss = 50.36064896\n",
      "Iteration 366, loss = 50.23238545\n",
      "Iteration 367, loss = 50.11812541\n",
      "Iteration 368, loss = 49.99483096\n",
      "Iteration 369, loss = 49.87063303\n",
      "Iteration 370, loss = 49.76194148\n",
      "Iteration 371, loss = 49.65312011\n",
      "Iteration 372, loss = 49.54679168\n",
      "Iteration 373, loss = 49.43317286\n",
      "Iteration 374, loss = 49.32486266\n",
      "Iteration 375, loss = 49.21823904\n",
      "Iteration 376, loss = 49.10426793\n",
      "Iteration 377, loss = 48.98862322\n",
      "Iteration 378, loss = 48.87124118\n",
      "Iteration 379, loss = 48.75881861\n",
      "Iteration 380, loss = 48.64508612\n",
      "Iteration 381, loss = 48.53123964\n",
      "Iteration 382, loss = 48.41072866\n",
      "Iteration 383, loss = 48.30103637\n",
      "Iteration 384, loss = 48.18185675\n",
      "Iteration 385, loss = 48.07603672\n",
      "Iteration 386, loss = 47.97098321\n",
      "Iteration 387, loss = 47.86789211\n",
      "Iteration 388, loss = 47.76037540\n",
      "Iteration 389, loss = 47.65058161\n",
      "Iteration 390, loss = 47.52800595\n",
      "Iteration 391, loss = 47.39561456\n",
      "Iteration 392, loss = 47.24813978\n",
      "Iteration 393, loss = 47.10720309\n",
      "Iteration 394, loss = 46.97907675\n",
      "Iteration 395, loss = 46.85958630\n",
      "Iteration 396, loss = 46.73873542\n",
      "Iteration 397, loss = 46.62520040\n",
      "Iteration 398, loss = 46.49668848\n",
      "Iteration 399, loss = 46.37685450\n",
      "Iteration 400, loss = 46.24892265\n",
      "Iteration 401, loss = 46.12157032\n",
      "Iteration 402, loss = 46.00254628\n",
      "Iteration 403, loss = 45.88433430\n",
      "Iteration 404, loss = 45.76733678\n",
      "Iteration 405, loss = 45.65735417\n",
      "Iteration 406, loss = 45.54296735\n",
      "Iteration 407, loss = 45.42932434\n",
      "Iteration 408, loss = 45.31062053\n",
      "Iteration 409, loss = 45.18397689\n",
      "Iteration 410, loss = 45.06268824\n",
      "Iteration 411, loss = 44.94134702\n",
      "Iteration 412, loss = 44.81666610\n",
      "Iteration 413, loss = 44.69449751\n",
      "Iteration 414, loss = 44.57246392\n",
      "Iteration 415, loss = 44.44062710\n",
      "Iteration 416, loss = 44.32507225\n",
      "Iteration 417, loss = 44.21092164\n",
      "Iteration 418, loss = 44.09170629\n",
      "Iteration 419, loss = 43.95745901\n",
      "Iteration 420, loss = 43.82246560\n",
      "Iteration 421, loss = 43.69745787\n",
      "Iteration 422, loss = 43.57798320\n",
      "Iteration 423, loss = 43.46893898\n",
      "Iteration 424, loss = 43.35172104\n",
      "Iteration 425, loss = 43.23273294\n",
      "Iteration 426, loss = 43.12547431\n",
      "Iteration 427, loss = 43.02229642\n",
      "Iteration 428, loss = 42.92527381\n",
      "Iteration 429, loss = 42.83703294\n",
      "Iteration 430, loss = 42.74640412\n",
      "Iteration 431, loss = 42.64715946\n",
      "Iteration 432, loss = 42.54486994\n",
      "Iteration 433, loss = 42.43579849\n",
      "Iteration 434, loss = 42.32986645\n",
      "Iteration 435, loss = 42.23417155\n",
      "Iteration 436, loss = 42.13909891\n",
      "Iteration 437, loss = 42.04467367\n",
      "Iteration 438, loss = 41.93946421\n",
      "Iteration 439, loss = 41.82877618\n",
      "Iteration 440, loss = 41.72854070\n",
      "Iteration 441, loss = 41.62729828\n",
      "Iteration 442, loss = 41.53230179\n",
      "Iteration 443, loss = 41.43652732\n",
      "Iteration 444, loss = 41.34582936\n",
      "Iteration 445, loss = 41.25769239\n",
      "Iteration 446, loss = 41.17085316\n",
      "Iteration 447, loss = 41.07820157\n",
      "Iteration 448, loss = 40.98267197\n",
      "Iteration 449, loss = 40.88903519\n",
      "Iteration 450, loss = 40.78134666\n",
      "Iteration 451, loss = 40.65829300\n",
      "Iteration 452, loss = 40.53057162\n",
      "Iteration 453, loss = 40.41338690\n",
      "Iteration 454, loss = 40.30815137\n",
      "Iteration 455, loss = 40.20857099\n",
      "Iteration 456, loss = 40.10664337\n",
      "Iteration 457, loss = 40.00099847\n",
      "Iteration 458, loss = 39.89052644\n",
      "Iteration 459, loss = 39.77502965\n",
      "Iteration 460, loss = 39.67138950\n",
      "Iteration 461, loss = 39.57226171\n",
      "Iteration 462, loss = 39.47483925\n",
      "Iteration 463, loss = 39.37016036\n",
      "Iteration 464, loss = 39.25629904\n",
      "Iteration 465, loss = 39.15380717\n",
      "Iteration 466, loss = 39.05875738\n",
      "Iteration 467, loss = 38.96926519\n",
      "Iteration 468, loss = 38.87580826\n",
      "Iteration 469, loss = 38.78315077\n",
      "Iteration 470, loss = 38.69454239\n",
      "Iteration 471, loss = 38.60079498\n",
      "Iteration 472, loss = 38.51632352\n",
      "Iteration 473, loss = 38.42646637\n",
      "Iteration 474, loss = 38.34679584\n",
      "Iteration 475, loss = 38.26720327\n",
      "Iteration 476, loss = 38.19398207\n",
      "Iteration 477, loss = 38.11173196\n",
      "Iteration 478, loss = 38.03016603\n",
      "Iteration 479, loss = 37.94319358\n",
      "Iteration 480, loss = 37.86157619\n",
      "Iteration 481, loss = 37.77779752\n",
      "Iteration 482, loss = 37.69208269\n",
      "Iteration 483, loss = 37.60020661\n",
      "Iteration 484, loss = 37.49705165\n",
      "Iteration 485, loss = 37.39994981\n",
      "Iteration 486, loss = 37.30109105\n",
      "Iteration 487, loss = 37.21195079\n",
      "Iteration 488, loss = 37.12594907\n",
      "Iteration 489, loss = 37.04574340\n",
      "Iteration 490, loss = 36.95800574\n",
      "Iteration 491, loss = 36.87476154\n",
      "Iteration 492, loss = 36.79426349\n",
      "Iteration 493, loss = 36.71827238\n",
      "Iteration 494, loss = 36.64165581\n",
      "Iteration 495, loss = 36.55366505\n",
      "Iteration 496, loss = 36.45862884\n",
      "Iteration 497, loss = 36.37133727\n",
      "Iteration 498, loss = 36.29641176\n",
      "Iteration 499, loss = 36.21831253\n",
      "Iteration 500, loss = 36.13936551\n",
      "Iteration 501, loss = 36.05987683\n",
      "Iteration 502, loss = 35.97529240\n",
      "Iteration 503, loss = 35.88190612\n",
      "Iteration 504, loss = 35.79128497\n",
      "Iteration 505, loss = 35.70007980\n",
      "Iteration 506, loss = 35.60204512\n",
      "Iteration 507, loss = 35.51158850\n",
      "Iteration 508, loss = 35.41673268\n",
      "Iteration 509, loss = 35.33063991\n",
      "Iteration 510, loss = 35.24634388\n",
      "Iteration 511, loss = 35.16997655\n",
      "Iteration 512, loss = 35.09407270\n",
      "Iteration 513, loss = 35.01017146\n",
      "Iteration 514, loss = 34.93072087\n",
      "Iteration 515, loss = 34.85416731\n",
      "Iteration 516, loss = 34.77782013\n",
      "Iteration 517, loss = 34.69975463\n",
      "Iteration 518, loss = 34.62899255\n",
      "Iteration 519, loss = 34.56066521\n",
      "Iteration 520, loss = 34.49664390\n",
      "Iteration 521, loss = 34.41920453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 522, loss = 34.34436815\n",
      "Iteration 523, loss = 34.26654215\n",
      "Iteration 524, loss = 34.19861460\n",
      "Iteration 525, loss = 34.12596254\n",
      "Iteration 526, loss = 34.05403477\n",
      "Iteration 527, loss = 33.98060142\n",
      "Iteration 528, loss = 33.89683672\n",
      "Iteration 529, loss = 33.81625665\n",
      "Iteration 530, loss = 33.74136309\n",
      "Iteration 531, loss = 33.65597053\n",
      "Iteration 532, loss = 33.57063595\n",
      "Iteration 533, loss = 33.48658017\n",
      "Iteration 534, loss = 33.40035751\n",
      "Iteration 535, loss = 33.32315667\n",
      "Iteration 536, loss = 33.24513657\n",
      "Iteration 537, loss = 33.18453671\n",
      "Iteration 538, loss = 33.11416391\n",
      "Iteration 539, loss = 33.04918214\n",
      "Iteration 540, loss = 32.97733049\n",
      "Iteration 541, loss = 32.90065296\n",
      "Iteration 542, loss = 32.82612181\n",
      "Iteration 543, loss = 32.75140632\n",
      "Iteration 544, loss = 32.68474605\n",
      "Iteration 545, loss = 32.61383033\n",
      "Iteration 546, loss = 32.53557545\n",
      "Iteration 547, loss = 32.45865948\n",
      "Iteration 548, loss = 32.38519354\n",
      "Iteration 549, loss = 32.31931921\n",
      "Iteration 550, loss = 32.25763907\n",
      "Iteration 551, loss = 32.18810701\n",
      "Iteration 552, loss = 32.13138491\n",
      "Iteration 553, loss = 32.07679461\n",
      "Iteration 554, loss = 32.02408361\n",
      "Iteration 555, loss = 31.97040988\n",
      "Iteration 556, loss = 31.92100537\n",
      "Iteration 557, loss = 31.85572602\n",
      "Iteration 558, loss = 31.79231778\n",
      "Iteration 559, loss = 31.73249457\n",
      "Iteration 560, loss = 31.67757311\n",
      "Iteration 561, loss = 31.62838153\n",
      "Iteration 562, loss = 31.59744140\n",
      "Iteration 563, loss = 31.52472849\n",
      "Iteration 564, loss = 31.44144828\n",
      "Iteration 565, loss = 31.35599352\n",
      "Iteration 566, loss = 31.27492467\n",
      "Iteration 567, loss = 31.21284439\n",
      "Iteration 568, loss = 31.15541690\n",
      "Iteration 569, loss = 31.10918507\n",
      "Iteration 570, loss = 31.06518631\n",
      "Iteration 571, loss = 31.02535812\n",
      "Iteration 572, loss = 30.98687031\n",
      "Iteration 573, loss = 30.94880266\n",
      "Iteration 574, loss = 30.90451774\n",
      "Iteration 575, loss = 30.86347174\n",
      "Iteration 576, loss = 30.80887036\n",
      "Iteration 577, loss = 30.75999233\n",
      "Iteration 578, loss = 30.70266483\n",
      "Iteration 579, loss = 30.65394291\n",
      "Iteration 580, loss = 30.58781801\n",
      "Iteration 581, loss = 30.52952998\n",
      "Iteration 582, loss = 30.47537920\n",
      "Iteration 583, loss = 30.42202993\n",
      "Iteration 584, loss = 30.36836504\n",
      "Iteration 585, loss = 30.32367562\n",
      "Iteration 586, loss = 30.28559111\n",
      "Iteration 587, loss = 30.24515798\n",
      "Iteration 588, loss = 30.20399756\n",
      "Iteration 589, loss = 30.16673812\n",
      "Iteration 590, loss = 30.13006485\n",
      "Iteration 591, loss = 30.09574402\n",
      "Iteration 592, loss = 30.06458282\n",
      "Iteration 593, loss = 30.02490699\n",
      "Iteration 594, loss = 29.96996920\n",
      "Iteration 595, loss = 29.91364101\n",
      "Iteration 596, loss = 29.86978137\n",
      "Iteration 597, loss = 29.82844441\n",
      "Iteration 598, loss = 29.79088806\n",
      "Iteration 599, loss = 29.74957373\n",
      "Iteration 600, loss = 29.69751443\n",
      "Iteration 601, loss = 29.64008466\n",
      "Iteration 602, loss = 29.58460702\n",
      "Iteration 603, loss = 29.52984642\n",
      "Iteration 604, loss = 29.47564765\n",
      "Iteration 605, loss = 29.42670701\n",
      "Iteration 606, loss = 29.37921175\n",
      "Iteration 607, loss = 29.33632056\n",
      "Iteration 608, loss = 29.29193258\n",
      "Iteration 609, loss = 29.25819117\n",
      "Iteration 610, loss = 29.22131351\n",
      "Iteration 611, loss = 29.19009881\n",
      "Iteration 612, loss = 29.15851331\n",
      "Iteration 613, loss = 29.12631887\n",
      "Iteration 614, loss = 29.07746766\n",
      "Iteration 615, loss = 29.02663324\n",
      "Iteration 616, loss = 28.98476915\n",
      "Iteration 617, loss = 28.94187524\n",
      "Iteration 618, loss = 28.90202169\n",
      "Iteration 619, loss = 28.86638375\n",
      "Iteration 620, loss = 28.82822083\n",
      "Iteration 621, loss = 28.78976123\n",
      "Iteration 622, loss = 28.75432547\n",
      "Iteration 623, loss = 28.71446201\n",
      "Iteration 624, loss = 28.67554307\n",
      "Iteration 625, loss = 28.63788735\n",
      "Iteration 626, loss = 28.59913336\n",
      "Iteration 627, loss = 28.56539061\n",
      "Iteration 628, loss = 28.53401767\n",
      "Iteration 629, loss = 28.49269665\n",
      "Iteration 630, loss = 28.45379301\n",
      "Iteration 631, loss = 28.41771211\n",
      "Iteration 632, loss = 28.37462236\n",
      "Iteration 633, loss = 28.32894007\n",
      "Iteration 634, loss = 28.28190066\n",
      "Iteration 635, loss = 28.24118408\n",
      "Iteration 636, loss = 28.20361364\n",
      "Iteration 637, loss = 28.17173931\n",
      "Iteration 638, loss = 28.14442149\n",
      "Iteration 639, loss = 28.11427913\n",
      "Iteration 640, loss = 28.08321670\n",
      "Iteration 641, loss = 28.03735542\n",
      "Iteration 642, loss = 28.01178871\n",
      "Iteration 643, loss = 27.97111548\n",
      "Iteration 644, loss = 27.93580434\n",
      "Iteration 645, loss = 27.89998277\n",
      "Iteration 646, loss = 27.86587264\n",
      "Iteration 647, loss = 27.83179040\n",
      "Iteration 648, loss = 27.79358729\n",
      "Iteration 649, loss = 27.75284662\n",
      "Iteration 650, loss = 27.71671279\n",
      "Iteration 651, loss = 27.68677224\n",
      "Iteration 652, loss = 27.65710326\n",
      "Iteration 653, loss = 27.63382367\n",
      "Iteration 654, loss = 27.60831903\n",
      "Iteration 655, loss = 27.58118969\n",
      "Iteration 656, loss = 27.54603244\n",
      "Iteration 657, loss = 27.51354335\n",
      "Iteration 658, loss = 27.48173038\n",
      "Iteration 659, loss = 27.45111965\n",
      "Iteration 660, loss = 27.41800322\n",
      "Iteration 661, loss = 27.38313660\n",
      "Iteration 662, loss = 27.35120661\n",
      "Iteration 663, loss = 27.31975893\n",
      "Iteration 664, loss = 27.28743836\n",
      "Iteration 665, loss = 27.24937107\n",
      "Iteration 666, loss = 27.21944606\n",
      "Iteration 667, loss = 27.18118212\n",
      "Iteration 668, loss = 27.14965359\n",
      "Iteration 669, loss = 27.11047709\n",
      "Iteration 670, loss = 27.07976111\n",
      "Iteration 671, loss = 27.04684020\n",
      "Iteration 672, loss = 27.02362028\n",
      "Iteration 673, loss = 27.01836250\n",
      "Iteration 674, loss = 26.99771144\n",
      "Iteration 675, loss = 26.97948998\n",
      "Iteration 676, loss = 26.94859446\n",
      "Iteration 677, loss = 26.93047243\n",
      "Iteration 678, loss = 26.90621203\n",
      "Iteration 679, loss = 26.88146074\n",
      "Iteration 680, loss = 26.85305835\n",
      "Iteration 681, loss = 26.82108368\n",
      "Iteration 682, loss = 26.78850192\n",
      "Iteration 683, loss = 26.75524897\n",
      "Iteration 684, loss = 26.72391153\n",
      "Iteration 685, loss = 26.69126807\n",
      "Iteration 686, loss = 26.65880663\n",
      "Iteration 687, loss = 26.62502306\n",
      "Iteration 688, loss = 26.60556179\n",
      "Iteration 689, loss = 26.58028164\n",
      "Iteration 690, loss = 26.54389330\n",
      "Iteration 691, loss = 26.50591050\n",
      "Iteration 692, loss = 26.45656353\n",
      "Iteration 693, loss = 26.40203078\n",
      "Iteration 694, loss = 26.36495105\n",
      "Iteration 695, loss = 26.32905823\n",
      "Iteration 696, loss = 26.30869617\n",
      "Iteration 697, loss = 26.27942625\n",
      "Iteration 698, loss = 26.25659926\n",
      "Iteration 699, loss = 26.23667099\n",
      "Iteration 700, loss = 26.21330836\n",
      "Iteration 701, loss = 26.19830884\n",
      "Iteration 702, loss = 26.18268382\n",
      "Iteration 703, loss = 26.16534907\n",
      "Iteration 704, loss = 26.16151820\n",
      "Iteration 705, loss = 26.15820765\n",
      "Iteration 706, loss = 26.14038474\n",
      "Iteration 707, loss = 26.11316019\n",
      "Iteration 708, loss = 26.09101469\n",
      "Iteration 709, loss = 26.06551455\n",
      "Iteration 710, loss = 26.04664199\n",
      "Iteration 711, loss = 26.02184845\n",
      "Iteration 712, loss = 25.99866330\n",
      "Iteration 713, loss = 25.97232826\n",
      "Iteration 714, loss = 25.94976822\n",
      "Iteration 715, loss = 25.92921589\n",
      "Iteration 716, loss = 25.90648590\n",
      "Iteration 717, loss = 25.88216991\n",
      "Iteration 718, loss = 25.86042665\n",
      "Iteration 719, loss = 25.83932009\n",
      "Iteration 720, loss = 25.82224631\n",
      "Iteration 721, loss = 25.80253994\n",
      "Iteration 722, loss = 25.77214633\n",
      "Iteration 723, loss = 25.73414012\n",
      "Iteration 724, loss = 25.69358272\n",
      "Iteration 725, loss = 25.64697377\n",
      "Iteration 726, loss = 25.60943077\n",
      "Iteration 727, loss = 25.58002043\n",
      "Iteration 728, loss = 25.55495123\n",
      "Iteration 729, loss = 25.52446658\n",
      "Iteration 730, loss = 25.50243237\n",
      "Iteration 731, loss = 25.47000859\n",
      "Iteration 732, loss = 25.44521490\n",
      "Iteration 733, loss = 25.42108418\n",
      "Iteration 734, loss = 25.40105165\n",
      "Iteration 735, loss = 25.37558890\n",
      "Iteration 736, loss = 25.35151242\n",
      "Iteration 737, loss = 25.32807678\n",
      "Iteration 738, loss = 25.30875122\n",
      "Iteration 739, loss = 25.28496792\n",
      "Iteration 740, loss = 25.24830915\n",
      "Iteration 741, loss = 25.21476484\n",
      "Iteration 742, loss = 25.18624671\n",
      "Iteration 743, loss = 25.15513026\n",
      "Iteration 744, loss = 25.12995266\n",
      "Iteration 745, loss = 25.10131268\n",
      "Iteration 746, loss = 25.06768369\n",
      "Iteration 747, loss = 25.03486734\n",
      "Iteration 748, loss = 25.00366316\n",
      "Iteration 749, loss = 24.98270887\n",
      "Iteration 750, loss = 24.95686821\n",
      "Iteration 751, loss = 24.93356977\n",
      "Iteration 752, loss = 24.90786199\n",
      "Iteration 753, loss = 24.87917301\n",
      "Iteration 754, loss = 24.85383587\n",
      "Iteration 755, loss = 24.82461653\n",
      "Iteration 756, loss = 24.80283605\n",
      "Iteration 757, loss = 24.77975868\n",
      "Iteration 758, loss = 24.75669918\n",
      "Iteration 759, loss = 24.73330100\n",
      "Iteration 760, loss = 24.71071311\n",
      "Iteration 761, loss = 24.68617545\n",
      "Iteration 762, loss = 24.66298030\n",
      "Iteration 763, loss = 24.64270860\n",
      "Iteration 764, loss = 24.62490617\n",
      "Iteration 765, loss = 24.60527228\n",
      "Iteration 766, loss = 24.58324359\n",
      "Iteration 767, loss = 24.55281078\n",
      "Iteration 768, loss = 24.53005426\n",
      "Iteration 769, loss = 24.51121645\n",
      "Iteration 770, loss = 24.49603247\n",
      "Iteration 771, loss = 24.47934175\n",
      "Iteration 772, loss = 24.46211184\n",
      "Iteration 773, loss = 24.44103447\n",
      "Iteration 774, loss = 24.41677259\n",
      "Iteration 775, loss = 24.38897725\n",
      "Iteration 776, loss = 24.36151494\n",
      "Iteration 777, loss = 24.33671748\n",
      "Iteration 778, loss = 24.32214655\n",
      "Iteration 779, loss = 24.29927005\n",
      "Iteration 780, loss = 24.28005514\n",
      "Iteration 781, loss = 24.26023419\n",
      "Iteration 782, loss = 24.23393894\n",
      "Iteration 783, loss = 24.21173344\n",
      "Iteration 784, loss = 24.18746161\n",
      "Iteration 785, loss = 24.15551499\n",
      "Iteration 786, loss = 24.13732892\n",
      "Iteration 787, loss = 24.11468693\n",
      "Iteration 788, loss = 24.09928814\n",
      "Iteration 789, loss = 24.07638505\n",
      "Iteration 790, loss = 24.05013398\n",
      "Iteration 791, loss = 24.02310141\n",
      "Iteration 792, loss = 23.99475430\n",
      "Iteration 793, loss = 23.96815212\n",
      "Iteration 794, loss = 23.94335593\n",
      "Iteration 795, loss = 23.91963684\n",
      "Iteration 796, loss = 23.89574242\n",
      "Iteration 797, loss = 23.87409246\n",
      "Iteration 798, loss = 23.85881502\n",
      "Iteration 799, loss = 23.83953526\n",
      "Iteration 800, loss = 23.82239621\n",
      "Iteration 801, loss = 23.80655923\n",
      "Iteration 802, loss = 23.78927046\n",
      "Iteration 803, loss = 23.77187786\n",
      "Iteration 804, loss = 23.75757177\n",
      "Iteration 805, loss = 23.73577594\n",
      "Iteration 806, loss = 23.71024051\n",
      "Iteration 807, loss = 23.68375429\n",
      "Iteration 808, loss = 23.66039307\n",
      "Iteration 809, loss = 23.64030570\n",
      "Iteration 810, loss = 23.62241837\n",
      "Iteration 811, loss = 23.60482948\n",
      "Iteration 812, loss = 23.59016440\n",
      "Iteration 813, loss = 23.57402943\n",
      "Iteration 814, loss = 23.56084932\n",
      "Iteration 815, loss = 23.54968937\n",
      "Iteration 816, loss = 23.53711902\n",
      "Iteration 817, loss = 23.52488356\n",
      "Iteration 818, loss = 23.50883445\n",
      "Iteration 819, loss = 23.48615378\n",
      "Iteration 820, loss = 23.46492012\n",
      "Iteration 821, loss = 23.43967989\n",
      "Iteration 822, loss = 23.41068273\n",
      "Iteration 823, loss = 23.38306069\n",
      "Iteration 824, loss = 23.36285930\n",
      "Iteration 825, loss = 23.34416572\n",
      "Iteration 826, loss = 23.32636749\n",
      "Iteration 827, loss = 23.30625124\n",
      "Iteration 828, loss = 23.28579294\n",
      "Iteration 829, loss = 23.26468659\n",
      "Iteration 830, loss = 23.24184720\n",
      "Iteration 831, loss = 23.23113569\n",
      "Iteration 832, loss = 23.21659884\n",
      "Iteration 833, loss = 23.20167178\n",
      "Iteration 834, loss = 23.18554319\n",
      "Iteration 835, loss = 23.16349113\n",
      "Iteration 836, loss = 23.14230466\n",
      "Iteration 837, loss = 23.12086894\n",
      "Iteration 838, loss = 23.09933156\n",
      "Iteration 839, loss = 23.07554102\n",
      "Iteration 840, loss = 23.05702619\n",
      "Iteration 841, loss = 23.03640429\n",
      "Iteration 842, loss = 23.02037880\n",
      "Iteration 843, loss = 23.00425187\n",
      "Iteration 844, loss = 22.98958293\n",
      "Iteration 845, loss = 22.97995292\n",
      "Iteration 846, loss = 22.96899985\n",
      "Iteration 847, loss = 22.96026553\n",
      "Iteration 848, loss = 22.94982430\n",
      "Iteration 849, loss = 22.94187277\n",
      "Iteration 850, loss = 22.92713105\n",
      "Iteration 851, loss = 22.91301745\n",
      "Iteration 852, loss = 22.89133665\n",
      "Iteration 853, loss = 22.87041461\n",
      "Iteration 854, loss = 22.84693395\n",
      "Iteration 855, loss = 22.83317345\n",
      "Iteration 856, loss = 22.81632046\n",
      "Iteration 857, loss = 22.78864291\n",
      "Iteration 858, loss = 22.76779812\n",
      "Iteration 859, loss = 22.74990711\n",
      "Iteration 860, loss = 22.73248644\n",
      "Iteration 861, loss = 22.71663038\n",
      "Iteration 862, loss = 22.70218047\n",
      "Iteration 863, loss = 22.68164533\n",
      "Iteration 864, loss = 22.66449990\n",
      "Iteration 865, loss = 22.64920197\n",
      "Iteration 866, loss = 22.62719149\n",
      "Iteration 867, loss = 22.59583244\n",
      "Iteration 868, loss = 22.57225752\n",
      "Iteration 869, loss = 22.54696489\n",
      "Iteration 870, loss = 22.52736811\n",
      "Iteration 871, loss = 22.50337785\n",
      "Iteration 872, loss = 22.48277589\n",
      "Iteration 873, loss = 22.46450550\n",
      "Iteration 874, loss = 22.44919929\n",
      "Iteration 875, loss = 22.43236632\n",
      "Iteration 876, loss = 22.41326415\n",
      "Iteration 877, loss = 22.39537516\n",
      "Iteration 878, loss = 22.37696676\n",
      "Iteration 879, loss = 22.36091423\n",
      "Iteration 880, loss = 22.34321457\n",
      "Iteration 881, loss = 22.33178436\n",
      "Iteration 882, loss = 22.31463565\n",
      "Iteration 883, loss = 22.29984098\n",
      "Iteration 884, loss = 22.28435558\n",
      "Iteration 885, loss = 22.26497555\n",
      "Iteration 886, loss = 22.24503878\n",
      "Iteration 887, loss = 22.22732893\n",
      "Iteration 888, loss = 22.20715870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 889, loss = 22.18230100\n",
      "Iteration 890, loss = 22.16692880\n",
      "Iteration 891, loss = 22.15100113\n",
      "Iteration 892, loss = 22.13417826\n",
      "Iteration 893, loss = 22.11761099\n",
      "Iteration 894, loss = 22.10099051\n",
      "Iteration 895, loss = 22.08229225\n",
      "Iteration 896, loss = 22.07018018\n",
      "Iteration 897, loss = 22.06230408\n",
      "Iteration 898, loss = 22.04538079\n",
      "Iteration 899, loss = 22.02699661\n",
      "Iteration 900, loss = 22.01397818\n",
      "Iteration 901, loss = 22.00169981\n",
      "Iteration 902, loss = 21.98003562\n",
      "Iteration 903, loss = 21.95387619\n",
      "Iteration 904, loss = 21.92947657\n",
      "Iteration 905, loss = 21.90631612\n",
      "Iteration 906, loss = 21.88398050\n",
      "Iteration 907, loss = 21.86899152\n",
      "Iteration 908, loss = 21.85192281\n",
      "Iteration 909, loss = 21.83453455\n",
      "Iteration 910, loss = 21.82057545\n",
      "Iteration 911, loss = 21.81284713\n",
      "Iteration 912, loss = 21.79219620\n",
      "Iteration 913, loss = 21.76733411\n",
      "Iteration 914, loss = 21.73670276\n",
      "Iteration 915, loss = 21.70658386\n",
      "Iteration 916, loss = 21.67956188\n",
      "Iteration 917, loss = 21.65559595\n",
      "Iteration 918, loss = 21.63098748\n",
      "Iteration 919, loss = 21.61539385\n",
      "Iteration 920, loss = 21.59723985\n",
      "Iteration 921, loss = 21.58372902\n",
      "Iteration 922, loss = 21.56463782\n",
      "Iteration 923, loss = 21.55579179\n",
      "Iteration 924, loss = 21.54444255\n",
      "Iteration 925, loss = 21.52742562\n",
      "Iteration 926, loss = 21.51216481\n",
      "Iteration 927, loss = 21.50180325\n",
      "Iteration 928, loss = 21.49110201\n",
      "Iteration 929, loss = 21.47447478\n",
      "Iteration 930, loss = 21.44773229\n",
      "Iteration 931, loss = 21.42342402\n",
      "Iteration 932, loss = 21.40200214\n",
      "Iteration 933, loss = 21.38231533\n",
      "Iteration 934, loss = 21.36431960\n",
      "Iteration 935, loss = 21.33896381\n",
      "Iteration 936, loss = 21.32733595\n",
      "Iteration 937, loss = 21.33484477\n",
      "Iteration 938, loss = 21.32747152\n",
      "Iteration 939, loss = 21.30690491\n",
      "Iteration 940, loss = 21.27838336\n",
      "Iteration 941, loss = 21.25395074\n",
      "Iteration 942, loss = 21.22232926\n",
      "Iteration 943, loss = 21.21437616\n",
      "Iteration 944, loss = 21.22309476\n",
      "Iteration 945, loss = 21.21246539\n",
      "Iteration 946, loss = 21.18078163\n",
      "Iteration 947, loss = 21.14147436\n",
      "Iteration 948, loss = 21.11006507\n",
      "Iteration 949, loss = 21.07343285\n",
      "Iteration 950, loss = 21.05123115\n",
      "Iteration 951, loss = 21.02426000\n",
      "Iteration 952, loss = 21.00059224\n",
      "Iteration 953, loss = 20.98590213\n",
      "Iteration 954, loss = 20.96185995\n",
      "Iteration 955, loss = 20.94158977\n",
      "Iteration 956, loss = 20.92275186\n",
      "Iteration 957, loss = 20.89930847\n",
      "Iteration 958, loss = 20.88067742\n",
      "Iteration 959, loss = 20.87294095\n",
      "Iteration 960, loss = 20.86011550\n",
      "Iteration 961, loss = 20.84364562\n",
      "Iteration 962, loss = 20.82179044\n",
      "Iteration 963, loss = 20.80210849\n",
      "Iteration 964, loss = 20.78476754\n",
      "Iteration 965, loss = 20.76646857\n",
      "Iteration 966, loss = 20.75260355\n",
      "Iteration 967, loss = 20.74236230\n",
      "Iteration 968, loss = 20.73133095\n",
      "Iteration 969, loss = 20.70847388\n",
      "Iteration 970, loss = 20.67761223\n",
      "Iteration 971, loss = 20.65345321\n",
      "Iteration 972, loss = 20.63453539\n",
      "Iteration 973, loss = 20.61883750\n",
      "Iteration 974, loss = 20.60501003\n",
      "Iteration 975, loss = 20.58887890\n",
      "Iteration 976, loss = 20.57354593\n",
      "Iteration 977, loss = 20.55516949\n",
      "Iteration 978, loss = 20.53806214\n",
      "Iteration 979, loss = 20.52546448\n",
      "Iteration 980, loss = 20.50903837\n",
      "Iteration 981, loss = 20.49425909\n",
      "Iteration 982, loss = 20.48123656\n",
      "Iteration 983, loss = 20.46842813\n",
      "Iteration 984, loss = 20.45373144\n",
      "Iteration 985, loss = 20.44013705\n",
      "Iteration 986, loss = 20.42248928\n",
      "Iteration 987, loss = 20.39638440\n",
      "Iteration 988, loss = 20.37137140\n",
      "Iteration 989, loss = 20.35332113\n",
      "Iteration 990, loss = 20.33398300\n",
      "Iteration 991, loss = 20.31817710\n",
      "Iteration 992, loss = 20.30207659\n",
      "Iteration 993, loss = 20.27979812\n",
      "Iteration 994, loss = 20.26570137\n",
      "Iteration 995, loss = 20.25333689\n",
      "Iteration 996, loss = 20.23820847\n",
      "Iteration 997, loss = 20.22234524\n",
      "Iteration 998, loss = 20.20647135\n",
      "Iteration 999, loss = 20.18750629\n",
      "Iteration 1000, loss = 20.16332913\n",
      "Iteration 1001, loss = 20.13407549\n",
      "Iteration 1002, loss = 20.11519129\n",
      "Iteration 1003, loss = 20.10854511\n",
      "Iteration 1004, loss = 20.09471445\n",
      "Iteration 1005, loss = 20.08892590\n",
      "Iteration 1006, loss = 20.07948041\n",
      "Iteration 1007, loss = 20.06456196\n",
      "Iteration 1008, loss = 20.04652498\n",
      "Iteration 1009, loss = 20.02633530\n",
      "Iteration 1010, loss = 20.00850855\n",
      "Iteration 1011, loss = 20.00313183\n",
      "Iteration 1012, loss = 19.99207826\n",
      "Iteration 1013, loss = 19.97984040\n",
      "Iteration 1014, loss = 19.96929074\n",
      "Iteration 1015, loss = 19.94839706\n",
      "Iteration 1016, loss = 19.92082748\n",
      "Iteration 1017, loss = 19.89442273\n",
      "Iteration 1018, loss = 19.85934668\n",
      "Iteration 1019, loss = 19.82942252\n",
      "Iteration 1020, loss = 19.80268110\n",
      "Iteration 1021, loss = 19.77767362\n",
      "Iteration 1022, loss = 19.75711987\n",
      "Iteration 1023, loss = 19.73546586\n",
      "Iteration 1024, loss = 19.72421121\n",
      "Iteration 1025, loss = 19.70902865\n",
      "Iteration 1026, loss = 19.69344139\n",
      "Iteration 1027, loss = 19.67899248\n",
      "Iteration 1028, loss = 19.66602625\n",
      "Iteration 1029, loss = 19.65613363\n",
      "Iteration 1030, loss = 19.64257717\n",
      "Iteration 1031, loss = 19.61929752\n",
      "Iteration 1032, loss = 19.58966675\n",
      "Iteration 1033, loss = 19.56427977\n",
      "Iteration 1034, loss = 19.54378656\n",
      "Iteration 1035, loss = 19.52028083\n",
      "Iteration 1036, loss = 19.50389979\n",
      "Iteration 1037, loss = 19.49387104\n",
      "Iteration 1038, loss = 19.47640761\n",
      "Iteration 1039, loss = 19.46867215\n",
      "Iteration 1040, loss = 19.47657606\n",
      "Iteration 1041, loss = 19.47940281\n",
      "Iteration 1042, loss = 19.46852746\n",
      "Iteration 1043, loss = 19.44712233\n",
      "Iteration 1044, loss = 19.41789843\n",
      "Iteration 1045, loss = 19.38203696\n",
      "Iteration 1046, loss = 19.34654311\n",
      "Iteration 1047, loss = 19.31240803\n",
      "Iteration 1048, loss = 19.29273619\n",
      "Iteration 1049, loss = 19.27154711\n",
      "Iteration 1050, loss = 19.25436634\n",
      "Iteration 1051, loss = 19.23356147\n",
      "Iteration 1052, loss = 19.22483895\n",
      "Iteration 1053, loss = 19.20974300\n",
      "Iteration 1054, loss = 19.19822425\n",
      "Iteration 1055, loss = 19.18527278\n",
      "Iteration 1056, loss = 19.17419409\n",
      "Iteration 1057, loss = 19.16183921\n",
      "Iteration 1058, loss = 19.14984052\n",
      "Iteration 1059, loss = 19.13851707\n",
      "Iteration 1060, loss = 19.12773886\n",
      "Iteration 1061, loss = 19.12382085\n",
      "Iteration 1062, loss = 19.11513066\n",
      "Iteration 1063, loss = 19.11321032\n",
      "Iteration 1064, loss = 19.11108295\n",
      "Iteration 1065, loss = 19.09280484\n",
      "Iteration 1066, loss = 19.07541734\n",
      "Iteration 1067, loss = 19.05760538\n",
      "Iteration 1068, loss = 19.04032456\n",
      "Iteration 1069, loss = 19.01953179\n",
      "Iteration 1070, loss = 18.98972620\n",
      "Iteration 1071, loss = 18.96665598\n",
      "Iteration 1072, loss = 18.95710844\n",
      "Iteration 1073, loss = 18.93471381\n",
      "Iteration 1074, loss = 18.91981835\n",
      "Iteration 1075, loss = 18.90717893\n",
      "Iteration 1076, loss = 18.88903035\n",
      "Iteration 1077, loss = 18.87599110\n",
      "Iteration 1078, loss = 18.86047508\n",
      "Iteration 1079, loss = 18.84579903\n",
      "Iteration 1080, loss = 18.82667200\n",
      "Iteration 1081, loss = 18.81541683\n",
      "Iteration 1082, loss = 18.80292839\n",
      "Iteration 1083, loss = 18.78813384\n",
      "Iteration 1084, loss = 18.77255078\n",
      "Iteration 1085, loss = 18.75682834\n",
      "Iteration 1086, loss = 18.74240284\n",
      "Iteration 1087, loss = 18.72670797\n",
      "Iteration 1088, loss = 18.71239612\n",
      "Iteration 1089, loss = 18.70031208\n",
      "Iteration 1090, loss = 18.68753408\n",
      "Iteration 1091, loss = 18.68148435\n",
      "Iteration 1092, loss = 18.66823058\n",
      "Iteration 1093, loss = 18.66126751\n",
      "Iteration 1094, loss = 18.64605639\n",
      "Iteration 1095, loss = 18.63279251\n",
      "Iteration 1096, loss = 18.60591476\n",
      "Iteration 1097, loss = 18.58993588\n",
      "Iteration 1098, loss = 18.57780349\n",
      "Iteration 1099, loss = 18.56289353\n",
      "Iteration 1100, loss = 18.57028204\n",
      "Iteration 1101, loss = 18.57002930\n",
      "Iteration 1102, loss = 18.54686515\n",
      "Iteration 1103, loss = 18.51747880\n",
      "Iteration 1104, loss = 18.49435629\n",
      "Iteration 1105, loss = 18.46975680\n",
      "Iteration 1106, loss = 18.45394456\n",
      "Iteration 1107, loss = 18.43225708\n",
      "Iteration 1108, loss = 18.41823750\n",
      "Iteration 1109, loss = 18.40249187\n",
      "Iteration 1110, loss = 18.38759624\n",
      "Iteration 1111, loss = 18.37173893\n",
      "Iteration 1112, loss = 18.35463844\n",
      "Iteration 1113, loss = 18.32849071\n",
      "Iteration 1114, loss = 18.31473635\n",
      "Iteration 1115, loss = 18.29772647\n",
      "Iteration 1116, loss = 18.28352770\n",
      "Iteration 1117, loss = 18.27024063\n",
      "Iteration 1118, loss = 18.25598363\n",
      "Iteration 1119, loss = 18.24317190\n",
      "Iteration 1120, loss = 18.23093594\n",
      "Iteration 1121, loss = 18.21753206\n",
      "Iteration 1122, loss = 18.20523209\n",
      "Iteration 1123, loss = 18.20015104\n",
      "Iteration 1124, loss = 18.18850413\n",
      "Iteration 1125, loss = 18.17460126\n",
      "Iteration 1126, loss = 18.16094109\n",
      "Iteration 1127, loss = 18.15141176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1128, loss = 18.14735353\n",
      "Iteration 1129, loss = 18.14531746\n",
      "Iteration 1130, loss = 18.14994678\n",
      "Iteration 1131, loss = 18.17215223\n",
      "Iteration 1132, loss = 18.16225328\n",
      "Iteration 1133, loss = 18.12333171\n",
      "Iteration 1134, loss = 18.09179200\n",
      "Iteration 1135, loss = 18.07127403\n",
      "Iteration 1136, loss = 18.04967908\n",
      "Iteration 1137, loss = 18.02269542\n",
      "Iteration 1138, loss = 17.99455515\n",
      "Iteration 1139, loss = 17.96935864\n",
      "Iteration 1140, loss = 17.94357981\n",
      "Iteration 1141, loss = 17.92287042\n",
      "Iteration 1142, loss = 17.90144577\n",
      "Iteration 1143, loss = 17.88650605\n",
      "Iteration 1144, loss = 17.87152487\n",
      "Iteration 1145, loss = 17.85823607\n",
      "Iteration 1146, loss = 17.84469319\n",
      "Iteration 1147, loss = 17.83588761\n",
      "Iteration 1148, loss = 17.82881257\n",
      "Iteration 1149, loss = 17.81771191\n",
      "Iteration 1150, loss = 17.80232910\n",
      "Iteration 1151, loss = 17.78699171\n",
      "Iteration 1152, loss = 17.77318969\n",
      "Iteration 1153, loss = 17.76137096\n",
      "Iteration 1154, loss = 17.75129750\n",
      "Iteration 1155, loss = 17.73954805\n",
      "Iteration 1156, loss = 17.73196311\n",
      "Iteration 1157, loss = 17.72668235\n",
      "Iteration 1158, loss = 17.71306126\n",
      "Iteration 1159, loss = 17.69715521\n",
      "Iteration 1160, loss = 17.69751190\n",
      "Iteration 1161, loss = 17.68784297\n",
      "Iteration 1162, loss = 17.66967882\n",
      "Iteration 1163, loss = 17.65357762\n",
      "Iteration 1164, loss = 17.63678184\n",
      "Iteration 1165, loss = 17.62727575\n",
      "Iteration 1166, loss = 17.62128898\n",
      "Iteration 1167, loss = 17.60997331\n",
      "Iteration 1168, loss = 17.60508519\n",
      "Iteration 1169, loss = 17.59488192\n",
      "Iteration 1170, loss = 17.58340195\n",
      "Iteration 1171, loss = 17.56752081\n",
      "Iteration 1172, loss = 17.55428489\n",
      "Iteration 1173, loss = 17.53232899\n",
      "Iteration 1174, loss = 17.52110688\n",
      "Iteration 1175, loss = 17.51679574\n",
      "Iteration 1176, loss = 17.51315984\n",
      "Iteration 1177, loss = 17.50753277\n",
      "Iteration 1178, loss = 17.50073887\n",
      "Iteration 1179, loss = 17.48760660\n",
      "Iteration 1180, loss = 17.47114829\n",
      "Iteration 1181, loss = 17.45965896\n",
      "Iteration 1182, loss = 17.46644450\n",
      "Iteration 1183, loss = 17.45314004\n",
      "Iteration 1184, loss = 17.43578399\n",
      "Iteration 1185, loss = 17.41751541\n",
      "Iteration 1186, loss = 17.39390387\n",
      "Iteration 1187, loss = 17.36467369\n",
      "Iteration 1188, loss = 17.34173928\n",
      "Iteration 1189, loss = 17.31501980\n",
      "Iteration 1190, loss = 17.28120811\n",
      "Iteration 1191, loss = 17.24195488\n",
      "Iteration 1192, loss = 17.22128982\n",
      "Iteration 1193, loss = 17.19788109\n",
      "Iteration 1194, loss = 17.18062658\n",
      "Iteration 1195, loss = 17.16543074\n",
      "Iteration 1196, loss = 17.14272496\n",
      "Iteration 1197, loss = 17.12913857\n",
      "Iteration 1198, loss = 17.11381088\n",
      "Iteration 1199, loss = 17.10036690\n",
      "Iteration 1200, loss = 17.09417635\n",
      "Iteration 1201, loss = 17.07929832\n",
      "Iteration 1202, loss = 17.06548802\n",
      "Iteration 1203, loss = 17.04843234\n",
      "Iteration 1204, loss = 17.03218188\n",
      "Iteration 1205, loss = 17.01301835\n",
      "Iteration 1206, loss = 16.99313199\n",
      "Iteration 1207, loss = 16.97281530\n",
      "Iteration 1208, loss = 16.95411523\n",
      "Iteration 1209, loss = 16.93826242\n",
      "Iteration 1210, loss = 16.91949429\n",
      "Iteration 1211, loss = 16.90603032\n",
      "Iteration 1212, loss = 16.88744011\n",
      "Iteration 1213, loss = 16.87524254\n",
      "Iteration 1214, loss = 16.85985932\n",
      "Iteration 1215, loss = 16.84391458\n",
      "Iteration 1216, loss = 16.83301691\n",
      "Iteration 1217, loss = 16.82244537\n",
      "Iteration 1218, loss = 16.81330277\n",
      "Iteration 1219, loss = 16.80323309\n",
      "Iteration 1220, loss = 16.79217253\n",
      "Iteration 1221, loss = 16.78298128\n",
      "Iteration 1222, loss = 16.77569213\n",
      "Iteration 1223, loss = 16.76567113\n",
      "Iteration 1224, loss = 16.75400734\n",
      "Iteration 1225, loss = 16.73757174\n",
      "Iteration 1226, loss = 16.71940290\n",
      "Iteration 1227, loss = 16.70052659\n",
      "Iteration 1228, loss = 16.68458586\n",
      "Iteration 1229, loss = 16.67187944\n",
      "Iteration 1230, loss = 16.65569574\n",
      "Iteration 1231, loss = 16.64971032\n",
      "Iteration 1232, loss = 16.64286446\n",
      "Iteration 1233, loss = 16.63149504\n",
      "Iteration 1234, loss = 16.61618122\n",
      "Iteration 1235, loss = 16.60602498\n",
      "Iteration 1236, loss = 16.59712144\n",
      "Iteration 1237, loss = 16.58860858\n",
      "Iteration 1238, loss = 16.57768262\n",
      "Iteration 1239, loss = 16.56863696\n",
      "Iteration 1240, loss = 16.55699841\n",
      "Iteration 1241, loss = 16.54902813\n",
      "Iteration 1242, loss = 16.54439263\n",
      "Iteration 1243, loss = 16.53782061\n",
      "Iteration 1244, loss = 16.53416211\n",
      "Iteration 1245, loss = 16.52456654\n",
      "Iteration 1246, loss = 16.51263852\n",
      "Iteration 1247, loss = 16.49625308\n",
      "Iteration 1248, loss = 16.47237761\n",
      "Iteration 1249, loss = 16.45631308\n",
      "Iteration 1250, loss = 16.44455815\n",
      "Iteration 1251, loss = 16.43212385\n",
      "Iteration 1252, loss = 16.42159681\n",
      "Iteration 1253, loss = 16.41310896\n",
      "Iteration 1254, loss = 16.40381256\n",
      "Iteration 1255, loss = 16.39609659\n",
      "Iteration 1256, loss = 16.39135874\n",
      "Iteration 1257, loss = 16.38607182\n",
      "Iteration 1258, loss = 16.38156798\n",
      "Iteration 1259, loss = 16.36998963\n",
      "Iteration 1260, loss = 16.35680280\n",
      "Iteration 1261, loss = 16.34075117\n",
      "Iteration 1262, loss = 16.32954273\n",
      "Iteration 1263, loss = 16.32114155\n",
      "Iteration 1264, loss = 16.29992966\n",
      "Iteration 1265, loss = 16.28258321\n",
      "Iteration 1266, loss = 16.27231495\n",
      "Iteration 1267, loss = 16.26477064\n",
      "Iteration 1268, loss = 16.25268219\n",
      "Iteration 1269, loss = 16.23678245\n",
      "Iteration 1270, loss = 16.25139945\n",
      "Iteration 1271, loss = 16.26201814\n",
      "Iteration 1272, loss = 16.26058573\n",
      "Iteration 1273, loss = 16.24904655\n",
      "Iteration 1274, loss = 16.23236823\n",
      "Iteration 1275, loss = 16.22031910\n",
      "Iteration 1276, loss = 16.20553985\n",
      "Iteration 1277, loss = 16.18699606\n",
      "Iteration 1278, loss = 16.17121391\n",
      "Iteration 1279, loss = 16.16925242\n",
      "Iteration 1280, loss = 16.16886039\n",
      "Iteration 1281, loss = 16.17558858\n",
      "Iteration 1282, loss = 16.16342895\n",
      "Iteration 1283, loss = 16.14081778\n",
      "Iteration 1284, loss = 16.11805350\n",
      "Iteration 1285, loss = 16.08561538\n",
      "Iteration 1286, loss = 16.05191220\n",
      "Iteration 1287, loss = 16.02879932\n",
      "Iteration 1288, loss = 16.01124444\n",
      "Iteration 1289, loss = 15.99749557\n",
      "Iteration 1290, loss = 15.98710808\n",
      "Iteration 1291, loss = 15.98116288\n",
      "Iteration 1292, loss = 15.97121915\n",
      "Iteration 1293, loss = 15.95582227\n",
      "Iteration 1294, loss = 15.94749370\n",
      "Iteration 1295, loss = 15.93985935\n",
      "Iteration 1296, loss = 15.93101394\n",
      "Iteration 1297, loss = 15.92452982\n",
      "Iteration 1298, loss = 15.91280238\n",
      "Iteration 1299, loss = 15.89812992\n",
      "Iteration 1300, loss = 15.88431167\n",
      "Iteration 1301, loss = 15.86863025\n",
      "Iteration 1302, loss = 15.85474305\n",
      "Iteration 1303, loss = 15.84257859\n",
      "Iteration 1304, loss = 15.83286123\n",
      "Iteration 1305, loss = 15.82127598\n",
      "Iteration 1306, loss = 15.80831513\n",
      "Iteration 1307, loss = 15.79452030\n",
      "Iteration 1308, loss = 15.78097284\n",
      "Iteration 1309, loss = 15.77271330\n",
      "Iteration 1310, loss = 15.76758205\n",
      "Iteration 1311, loss = 15.76502392\n",
      "Iteration 1312, loss = 15.77196669\n",
      "Iteration 1313, loss = 15.77777770\n",
      "Iteration 1314, loss = 15.79178863\n",
      "Iteration 1315, loss = 15.79306322\n",
      "Iteration 1316, loss = 15.77543728\n",
      "Iteration 1317, loss = 15.75490258\n",
      "Iteration 1318, loss = 15.73005684\n",
      "Iteration 1319, loss = 15.69677207\n",
      "Iteration 1320, loss = 15.66303119\n",
      "Iteration 1321, loss = 15.63760315\n",
      "Iteration 1322, loss = 15.61139637\n",
      "Iteration 1323, loss = 15.59089199\n",
      "Iteration 1324, loss = 15.57892369\n",
      "Iteration 1325, loss = 15.56435362\n",
      "Iteration 1326, loss = 15.55782055\n",
      "Iteration 1327, loss = 15.54730663\n",
      "Iteration 1328, loss = 15.54038486\n",
      "Iteration 1329, loss = 15.53491906\n",
      "Iteration 1330, loss = 15.54015461\n",
      "Iteration 1331, loss = 15.55173277\n",
      "Iteration 1332, loss = 15.55168751\n",
      "Iteration 1333, loss = 15.55191948\n",
      "Iteration 1334, loss = 15.52524364\n",
      "Iteration 1335, loss = 15.48986955\n",
      "Iteration 1336, loss = 15.46357279\n",
      "Iteration 1337, loss = 15.44234249\n",
      "Iteration 1338, loss = 15.42968899\n",
      "Iteration 1339, loss = 15.41014861\n",
      "Iteration 1340, loss = 15.39870078\n",
      "Iteration 1341, loss = 15.38609823\n",
      "Iteration 1342, loss = 15.37553459\n",
      "Iteration 1343, loss = 15.36554245\n",
      "Iteration 1344, loss = 15.35345072\n",
      "Iteration 1345, loss = 15.34448388\n",
      "Iteration 1346, loss = 15.33727635\n",
      "Iteration 1347, loss = 15.32596726\n",
      "Iteration 1348, loss = 15.30534616\n",
      "Iteration 1349, loss = 15.30387403\n",
      "Iteration 1350, loss = 15.29638377\n",
      "Iteration 1351, loss = 15.28596140\n",
      "Iteration 1352, loss = 15.27159783\n",
      "Iteration 1353, loss = 15.25746031\n",
      "Iteration 1354, loss = 15.24769498\n",
      "Iteration 1355, loss = 15.23204131\n",
      "Iteration 1356, loss = 15.22263063\n",
      "Iteration 1357, loss = 15.21296643\n",
      "Iteration 1358, loss = 15.20422517\n",
      "Iteration 1359, loss = 15.19644310\n",
      "Iteration 1360, loss = 15.19005193\n",
      "Iteration 1361, loss = 15.18436282\n",
      "Iteration 1362, loss = 15.17976418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1363, loss = 15.17461546\n",
      "Iteration 1364, loss = 15.17064014\n",
      "Iteration 1365, loss = 15.17029141\n",
      "Iteration 1366, loss = 15.16658150\n",
      "Iteration 1367, loss = 15.16903643\n",
      "Iteration 1368, loss = 15.16721827\n",
      "Iteration 1369, loss = 15.15132261\n",
      "Iteration 1370, loss = 15.12443674\n",
      "Iteration 1371, loss = 15.10732251\n",
      "Iteration 1372, loss = 15.09801275\n",
      "Iteration 1373, loss = 15.08327324\n",
      "Iteration 1374, loss = 15.07059066\n",
      "Iteration 1375, loss = 15.07324538\n",
      "Iteration 1376, loss = 15.07466774\n",
      "Iteration 1377, loss = 15.08193224\n",
      "Iteration 1378, loss = 15.09594301\n",
      "Iteration 1379, loss = 15.09708596\n",
      "Iteration 1380, loss = 15.08612558\n",
      "Iteration 1381, loss = 15.06560318\n",
      "Iteration 1382, loss = 15.04247502\n",
      "Iteration 1383, loss = 15.01582448\n",
      "Iteration 1384, loss = 14.99160410\n",
      "Iteration 1385, loss = 14.96935921\n",
      "Iteration 1386, loss = 14.94851280\n",
      "Iteration 1387, loss = 14.93433609\n",
      "Iteration 1388, loss = 14.92116589\n",
      "Iteration 1389, loss = 14.90620755\n",
      "Iteration 1390, loss = 14.89353046\n",
      "Iteration 1391, loss = 14.88508436\n",
      "Iteration 1392, loss = 14.86752393\n",
      "Iteration 1393, loss = 14.85641665\n",
      "Iteration 1394, loss = 14.84866808\n",
      "Iteration 1395, loss = 14.83579008\n",
      "Iteration 1396, loss = 14.82576293\n",
      "Iteration 1397, loss = 14.81549208\n",
      "Iteration 1398, loss = 14.81815507\n",
      "Iteration 1399, loss = 14.81045902\n",
      "Iteration 1400, loss = 14.81470834\n",
      "Iteration 1401, loss = 14.80904546\n",
      "Iteration 1402, loss = 14.80131788\n",
      "Iteration 1403, loss = 14.79214924\n",
      "Iteration 1404, loss = 14.77466273\n",
      "Iteration 1405, loss = 14.74222358\n",
      "Iteration 1406, loss = 14.70791083\n",
      "Iteration 1407, loss = 14.69454118\n",
      "Iteration 1408, loss = 14.68149861\n",
      "Iteration 1409, loss = 14.66973500\n",
      "Iteration 1410, loss = 14.66146642\n",
      "Iteration 1411, loss = 14.64732889\n",
      "Iteration 1412, loss = 14.63734638\n",
      "Iteration 1413, loss = 14.61814285\n",
      "Iteration 1414, loss = 14.60513954\n",
      "Iteration 1415, loss = 14.59335018\n",
      "Iteration 1416, loss = 14.58346797\n",
      "Iteration 1417, loss = 14.57649923\n",
      "Iteration 1418, loss = 14.56594674\n",
      "Iteration 1419, loss = 14.55627044\n",
      "Iteration 1420, loss = 14.55465565\n",
      "Iteration 1421, loss = 14.55949769\n",
      "Iteration 1422, loss = 14.55405739\n",
      "Iteration 1423, loss = 14.54703372\n",
      "Iteration 1424, loss = 14.53376512\n",
      "Iteration 1425, loss = 14.51914571\n",
      "Iteration 1426, loss = 14.50855706\n",
      "Iteration 1427, loss = 14.49819617\n",
      "Iteration 1428, loss = 14.48039165\n",
      "Iteration 1429, loss = 14.47021352\n",
      "Iteration 1430, loss = 14.46763705\n",
      "Iteration 1431, loss = 14.46553574\n",
      "Iteration 1432, loss = 14.46830368\n",
      "Iteration 1433, loss = 14.45798042\n",
      "Iteration 1434, loss = 14.44467722\n",
      "Iteration 1435, loss = 14.43158695\n",
      "Iteration 1436, loss = 14.42070046\n",
      "Iteration 1437, loss = 14.40708416\n",
      "Iteration 1438, loss = 14.40130563\n",
      "Iteration 1439, loss = 14.39066600\n",
      "Iteration 1440, loss = 14.38253898\n",
      "Iteration 1441, loss = 14.37334686\n",
      "Iteration 1442, loss = 14.35903769\n",
      "Iteration 1443, loss = 14.35484488\n",
      "Iteration 1444, loss = 14.34194536\n",
      "Iteration 1445, loss = 14.33002055\n",
      "Iteration 1446, loss = 14.31797439\n",
      "Iteration 1447, loss = 14.30845485\n",
      "Iteration 1448, loss = 14.29820737\n",
      "Iteration 1449, loss = 14.28783395\n",
      "Iteration 1450, loss = 14.28091843\n",
      "Iteration 1451, loss = 14.27183696\n",
      "Iteration 1452, loss = 14.26134177\n",
      "Iteration 1453, loss = 14.25186496\n",
      "Iteration 1454, loss = 14.24344018\n",
      "Iteration 1455, loss = 14.23730700\n",
      "Iteration 1456, loss = 14.23317017\n",
      "Iteration 1457, loss = 14.22594211\n",
      "Iteration 1458, loss = 14.22158636\n",
      "Iteration 1459, loss = 14.20845431\n",
      "Iteration 1460, loss = 14.20113040\n",
      "Iteration 1461, loss = 14.20569712\n",
      "Iteration 1462, loss = 14.20605523\n",
      "Iteration 1463, loss = 14.20012047\n",
      "Iteration 1464, loss = 14.18533936\n",
      "Iteration 1465, loss = 14.18055459\n",
      "Iteration 1466, loss = 14.17205242\n",
      "Iteration 1467, loss = 14.15963797\n",
      "Iteration 1468, loss = 14.15088412\n",
      "Iteration 1469, loss = 14.14517041\n",
      "Iteration 1470, loss = 14.13925245\n",
      "Iteration 1471, loss = 14.13384995\n",
      "Iteration 1472, loss = 14.12754942\n",
      "Iteration 1473, loss = 14.10128370\n",
      "Iteration 1474, loss = 14.09875629\n",
      "Iteration 1475, loss = 14.09305089\n",
      "Iteration 1476, loss = 14.09098653\n",
      "Iteration 1477, loss = 14.08965736\n",
      "Iteration 1478, loss = 14.08831368\n",
      "Iteration 1479, loss = 14.08582506\n",
      "Iteration 1480, loss = 14.08285151\n",
      "Iteration 1481, loss = 14.06799171\n",
      "Iteration 1482, loss = 14.06096143\n",
      "Iteration 1483, loss = 14.05207194\n",
      "Iteration 1484, loss = 14.04188970\n",
      "Iteration 1485, loss = 14.03325954\n",
      "Iteration 1486, loss = 14.02619900\n",
      "Iteration 1487, loss = 14.02085988\n",
      "Iteration 1488, loss = 14.01180379\n",
      "Iteration 1489, loss = 14.01297991\n",
      "Iteration 1490, loss = 14.00529677\n",
      "Iteration 1491, loss = 14.00106512\n",
      "Iteration 1492, loss = 14.00019302\n",
      "Iteration 1493, loss = 13.99693576\n",
      "Iteration 1494, loss = 13.99068666\n",
      "Iteration 1495, loss = 13.98063386\n",
      "Iteration 1496, loss = 13.97007018\n",
      "Iteration 1497, loss = 13.95353266\n",
      "Iteration 1498, loss = 13.94570073\n",
      "Iteration 1499, loss = 13.94656477\n",
      "Iteration 1500, loss = 13.94490195\n",
      "Iteration 1501, loss = 13.93809861\n",
      "Iteration 1502, loss = 13.92585706\n",
      "Iteration 1503, loss = 13.91740014\n",
      "Iteration 1504, loss = 13.90959554\n",
      "Iteration 1505, loss = 13.90516651\n",
      "Iteration 1506, loss = 13.89767560\n",
      "Iteration 1507, loss = 13.88392835\n",
      "Iteration 1508, loss = 13.87784007\n",
      "Iteration 1509, loss = 13.87505103\n",
      "Iteration 1510, loss = 13.87281513\n",
      "Iteration 1511, loss = 13.86463637\n",
      "Iteration 1512, loss = 13.86179435\n",
      "Iteration 1513, loss = 13.85594460\n",
      "Iteration 1514, loss = 13.85596018\n",
      "Iteration 1515, loss = 13.85408395\n",
      "Iteration 1516, loss = 13.86401105\n",
      "Iteration 1517, loss = 13.87222509\n",
      "Iteration 1518, loss = 13.88613506\n",
      "Iteration 1519, loss = 13.88326845\n",
      "Iteration 1520, loss = 13.86931068\n",
      "Iteration 1521, loss = 13.85878566\n",
      "Iteration 1522, loss = 13.83621475\n",
      "Iteration 1523, loss = 13.81905585\n",
      "Iteration 1524, loss = 13.80704027\n",
      "Iteration 1525, loss = 13.79727432\n",
      "Iteration 1526, loss = 13.79055511\n",
      "Iteration 1527, loss = 13.78420541\n",
      "Iteration 1528, loss = 13.78183344\n",
      "Iteration 1529, loss = 13.77835735\n",
      "Iteration 1530, loss = 13.77607332\n",
      "Iteration 1531, loss = 13.77682833\n",
      "Iteration 1532, loss = 13.77961604\n",
      "Iteration 1533, loss = 13.77479728\n",
      "Iteration 1534, loss = 13.76790005\n",
      "Iteration 1535, loss = 13.76846286\n",
      "Iteration 1536, loss = 13.76786656\n",
      "Iteration 1537, loss = 13.76063894\n",
      "Iteration 1538, loss = 13.75384095\n",
      "Iteration 1539, loss = 13.73953432\n",
      "Iteration 1540, loss = 13.72739852\n",
      "Iteration 1541, loss = 13.72071123\n",
      "Iteration 1542, loss = 13.71844560\n",
      "Iteration 1543, loss = 13.71692326\n",
      "Iteration 1544, loss = 13.71375920\n",
      "Iteration 1545, loss = 13.69653447\n",
      "Iteration 1546, loss = 13.68166993\n",
      "Iteration 1547, loss = 13.66765864\n",
      "Iteration 1548, loss = 13.66246093\n",
      "Iteration 1549, loss = 13.65806468\n",
      "Iteration 1550, loss = 13.65184056\n",
      "Iteration 1551, loss = 13.64584006\n",
      "Iteration 1552, loss = 13.64078734\n",
      "Iteration 1553, loss = 13.62837551\n",
      "Iteration 1554, loss = 13.63083068\n",
      "Iteration 1555, loss = 13.64677817\n",
      "Iteration 1556, loss = 13.63831396\n",
      "Iteration 1557, loss = 13.62916873\n",
      "Iteration 1558, loss = 13.61974281\n",
      "Iteration 1559, loss = 13.62094591\n",
      "Iteration 1560, loss = 13.62188337\n",
      "Iteration 1561, loss = 13.61513281\n",
      "Iteration 1562, loss = 13.60140736\n",
      "Iteration 1563, loss = 13.58341727\n",
      "Iteration 1564, loss = 13.57002547\n",
      "Iteration 1565, loss = 13.56680848\n",
      "Iteration 1566, loss = 13.55787707\n",
      "Iteration 1567, loss = 13.55559001\n",
      "Iteration 1568, loss = 13.54844179\n",
      "Iteration 1569, loss = 13.54365120\n",
      "Iteration 1570, loss = 13.53731972\n",
      "Iteration 1571, loss = 13.53096534\n",
      "Iteration 1572, loss = 13.52716510\n",
      "Iteration 1573, loss = 13.52447097\n",
      "Iteration 1574, loss = 13.51973857\n",
      "Iteration 1575, loss = 13.51743104\n",
      "Iteration 1576, loss = 13.51854373\n",
      "Iteration 1577, loss = 13.52001536\n",
      "Iteration 1578, loss = 13.53529236\n",
      "Iteration 1579, loss = 13.53907276\n",
      "Iteration 1580, loss = 13.53204074\n",
      "Iteration 1581, loss = 13.51453321\n",
      "Iteration 1582, loss = 13.50148922\n",
      "Iteration 1583, loss = 13.48867649\n",
      "Iteration 1584, loss = 13.47574023\n",
      "Iteration 1585, loss = 13.46292036\n",
      "Iteration 1586, loss = 13.45539159\n",
      "Iteration 1587, loss = 13.44764172\n",
      "Iteration 1588, loss = 13.43723110\n",
      "Iteration 1589, loss = 13.42901509\n",
      "Iteration 1590, loss = 13.43244126\n",
      "Iteration 1591, loss = 13.42796107\n",
      "Iteration 1592, loss = 13.42436393\n",
      "Iteration 1593, loss = 13.42010665\n",
      "Iteration 1594, loss = 13.41056641\n",
      "Iteration 1595, loss = 13.40081866\n",
      "Iteration 1596, loss = 13.39519731\n",
      "Iteration 1597, loss = 13.39053412\n",
      "Iteration 1598, loss = 13.38577917\n",
      "Iteration 1599, loss = 13.38028172\n",
      "Iteration 1600, loss = 13.37343858\n",
      "Iteration 1601, loss = 13.36768170\n",
      "Iteration 1602, loss = 13.36305965\n",
      "Iteration 1603, loss = 13.36069700\n",
      "Iteration 1604, loss = 13.35608346\n",
      "Iteration 1605, loss = 13.34721665\n",
      "Iteration 1606, loss = 13.34362213\n",
      "Iteration 1607, loss = 13.33974624\n",
      "Iteration 1608, loss = 13.33643186\n",
      "Iteration 1609, loss = 13.33101801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1610, loss = 13.32796446\n",
      "Iteration 1611, loss = 13.32577988\n",
      "Iteration 1612, loss = 13.32266378\n",
      "Iteration 1613, loss = 13.32100611\n",
      "Iteration 1614, loss = 13.31927377\n",
      "Iteration 1615, loss = 13.31355786\n",
      "Iteration 1616, loss = 13.30004637\n",
      "Iteration 1617, loss = 13.29422045\n",
      "Iteration 1618, loss = 13.28856014\n",
      "Iteration 1619, loss = 13.28576855\n",
      "Iteration 1620, loss = 13.27984307\n",
      "Iteration 1621, loss = 13.27914553\n",
      "Iteration 1622, loss = 13.26726967\n",
      "Iteration 1623, loss = 13.27976836\n",
      "Iteration 1624, loss = 13.30842206\n",
      "Iteration 1625, loss = 13.33083614\n",
      "Iteration 1626, loss = 13.33941237\n",
      "Iteration 1627, loss = 13.32314189\n",
      "Iteration 1628, loss = 13.31071264\n",
      "Iteration 1629, loss = 13.30569536\n",
      "Iteration 1630, loss = 13.36743180\n",
      "Iteration 1631, loss = 13.43736261\n",
      "Iteration 1632, loss = 13.48244929\n",
      "Iteration 1633, loss = 13.52050045\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=7, max_iter=2000, verbose=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = MLPRegressor(max_iter=2000, verbose=True, hidden_layer_sizes=(7))\n",
    "network.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'squared_loss'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do this with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast = datasets.load_breast_cancer()\n",
    "inputs = breast.data\n",
    "outputs = breast.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = nn.Sequential(nn.Linear(in_features=30, out_features=16),\n",
    "                        nn.Sigmoid(),\n",
    "                        nn.Linear(16, 16),\n",
    "                        nn.Sigmoid(),\n",
    "                        nn.Linear(16, 1),\n",
    "                        nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(network.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-154370bb259e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2516\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2517\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2518\u001b[1;33m         raise ValueError(\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\n\u001b[0m\u001b[0;32m   2519\u001b[0m                          \"Please ensure they have the same size.\".format(target.size(), input.size()))\n\u001b[0;32m   2520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.\n",
    "\n",
    "    for data in train_loader:\n",
    "        inputs, outputs = data\n",
    "        #print(inputs)\n",
    "        #print('-----')\n",
    "        #print(outputs)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = network.forward(inputs) \n",
    "        loss = loss_function(predictions, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch: ' + str(epoch + 1) + ' loss: ' + str(running_loss / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
